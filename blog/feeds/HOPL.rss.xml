<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>PRL Blog: Posts tagged 'HOPL'</title>
  <description>PRL Blog: Posts tagged 'HOPL'</description>
  <link>http://prl.ccs.neu.edu/blog/tags/HOPL.html</link>
  <lastBuildDate>Tue, 28 Feb 2017 09:51:55 UT</lastBuildDate>
  <pubDate>Tue, 28 Feb 2017 09:51:55 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>Linear Types for Low-level Languages</title>
   <link>http://prl.ccs.neu.edu/blog/2017/02/28/linear-types-for-low-level-languages/?utm_source=HOPL&amp;utm_medium=RSS</link>
   <guid>urn:http-prl-ccs-neu-edu:-blog-2017-02-28-linear-types-for-low-level-languages</guid>
   <pubDate>Tue, 28 Feb 2017 09:51:55 UT</pubDate>
   <description>&lt;!-- more--&gt;

&lt;p&gt;In this talk, we covered early papers (primarily, by Girard, Lafont, and Abramsky) on linear logic and its reflections into computation. The goal was to understand why linearity is often turned to as a principled way to control resource usage, as shows up in a language like Rust. From the very beginning, researchers realized the implications for &amp;ldquo;low-level&amp;rdquo; languages - that linear resources would eliminate the need for garbage collection, allow in-place mutation, and enable safe parallel computation. However, pure implementations of linearity incur lots of copying, doing away with any efficiency gained, and we covered a survey of papers that attempted to reconcile this contradiction by weakening linearity in controlled ways.&lt;/p&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/blob/master/lecture_notes/2017-02-14.md"&gt;https://github.com/nuprl/hopl-s2017/blob/master/lecture_notes/2017&amp;ndash;02&amp;ndash;14.md&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Just after the talk, over lunch, we had a lab discussion about the phrase &amp;ldquo;low level&amp;rdquo;. Here are some thoughts:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;the phrase is relative, both over time and depending on the programming  task at hand&lt;/li&gt;
 &lt;li&gt;a &amp;ldquo;low level&amp;rdquo; task is &amp;ldquo;one that you shouldn&amp;rsquo;t need to worry about&amp;rdquo; while  solving your current task&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;And here are some example &amp;ldquo;low-level&amp;rdquo; tasks:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Time and space management is &amp;ldquo;low level&amp;rdquo; when designing a new algorithm  (the first question is correctness)&lt;/li&gt;
 &lt;li&gt;Calling conventions and endian-ness (facets of the damn machine running  the programs) are almost always low-level&lt;/li&gt;
 &lt;li&gt;Whether a given value is serializable is usually low-level&lt;/li&gt;
 &lt;li&gt;Possible side effects, thrown exceptions, and optional arguments can all  be considered &amp;ldquo;low level&amp;rdquo; aspects of library functions. This is low-level  in the sense that &amp;ldquo;I&amp;rsquo;d rather use a simpler type to think about this library&amp;rdquo;&lt;/li&gt;
 &lt;li&gt;Managing type annotations is a low-level detail in ML programs&lt;/li&gt;&lt;/ul&gt;</description></item>
  <item>
   <title>Datalog for Static Analysis</title>
   <link>http://prl.ccs.neu.edu/blog/2017/02/21/datalog-for-static-analysis/?utm_source=HOPL&amp;utm_medium=RSS</link>
   <guid>urn:http-prl-ccs-neu-edu:-blog-2017-02-21-datalog-for-static-analysis</guid>
   <pubDate>Tue, 21 Feb 2017 12:58:27 UT</pubDate>
   <description>&lt;!-- more--&gt;

&lt;p&gt;Datalog is an old DSL that frequently appears in work on static analysis. This edition of &lt;a href="/blog/2017/02/15/introducing-hopl-2017/"&gt;HOPL 2017&lt;/a&gt; explores the origins of Datalog in general, its early use in program analysis, and why Datalog remains a useful tool.&lt;/p&gt;

&lt;p&gt;Full notes:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/blob/master/datalog-for-static-analysis/datalog-for-static-analysis.pdf"&gt;https://github.com/nuprl/hopl-s2017/blob/master/datalog-for-static-analysis/datalog-for-static-analysis.pdf&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Datalog as a language was introduced by 1978 (its semantic foundations date back to 1976). It is &lt;em&gt;predicate logic&lt;/em&gt; as a database query language. The traditional view of a Datalog program is a &lt;em&gt;time invariant&lt;/em&gt; transformation over the &lt;em&gt;time varying&lt;/em&gt; data stored in an external database.&lt;/p&gt;

&lt;p&gt;In the early 1990&amp;rsquo;s, Uwe Aβmann designed a graph rewriting systems (EARS) that could:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Uniformly express various problems in static analysis&lt;/li&gt;
 &lt;li&gt;Systematically derive efficient solutions to such problems.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;(Prior work had derived the same solutions with ad-hoc methods.) Aβmann&amp;rsquo;s system is equivalent to Datalog.&lt;/p&gt;

&lt;p&gt;In 1993, Reps used the 
 &lt;tt&gt;CORAL&lt;/tt&gt; deductive database (an implementation of Datalog) to derive an on-demand (read: lazy) implementation of program slicing from a &lt;em&gt;specification&lt;/em&gt; of the slicing problem.&lt;/p&gt;

&lt;p&gt;Both Aβmann&amp;rsquo;s and Reps work appeared in 1994. This was the first time Datalog had been used to implement a static analysis.&lt;/p&gt;

&lt;p&gt;Researchers continue to use Datalog because:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;predicate logic (specifically: Horn clauses without function symbols or negation)  is useful for expressing recursive relations &amp;hellip; and static analyses are all about recursive relations&lt;/li&gt;
 &lt;li&gt;the language separates &lt;em&gt;specifications&lt;/em&gt; from their &lt;em&gt;implementation&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;there are many techniques for efficiently serving a Datalog query&lt;/li&gt;
 &lt;li&gt;these techniques have been implemented in &lt;a href="https://developer.logicblox.com/wp-content/uploads/2016/01/logicblox-sigmod15.pdf"&gt;at least one&lt;/a&gt;  commercial Datalog engine&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;For an excellent description of how Datalog can benefit static analysis, see the introduction to &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.648.1834&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Rep&amp;rsquo;s paper&lt;/a&gt;.&lt;/p&gt;</description></item>
  <item>
   <title>Conversational Context and Concurrency</title>
   <link>http://prl.ccs.neu.edu/blog/2017/02/15/conversational-context-and-concurrency/?utm_source=HOPL&amp;utm_medium=RSS</link>
   <guid>urn:http-prl-ccs-neu-edu:-blog-2017-02-15-conversational-context-and-concurrency</guid>
   <pubDate>Wed, 15 Feb 2017 01:21:55 UT</pubDate>
   <description>&lt;!-- more--&gt;

&lt;p&gt;When programs are written with concurrency in mind, the programmer reasons about the interactions between concurrent components or agents in the program. This includes exchange of information, as well as management of resources, handling of partial failure, collective decision-making and so on.&lt;/p&gt;

&lt;p&gt;These components might be objects, or threads, or processes, or actors, or some more nebulous and loosely-defined concept; a group of callbacks, perhaps. The programmer has the notion of an agent in their mind, which translates into some representation of that agent in the program.&lt;/p&gt;

&lt;p&gt;We think about the contexts (because there can be more than one) in which agents exist in two different ways. From each agent&amp;rsquo;s perspective, the important thing to think about is the boundary between the agent and everything else in the system. But from the system perspective, we often think about &lt;em&gt;conversations&lt;/em&gt; between agents, whether it&amp;rsquo;s just two having an exchange, or a whole group collaborating on some task. Agents in a conversation play different roles, join and leave the group, and build shared conversational state.&lt;/p&gt;

&lt;p&gt;In this talk, I used the idea of these &lt;em&gt;conversational contexts&lt;/em&gt; as a lens through which to view the development of various metaphors and mechanisms of communication and coordination. I presented four &lt;em&gt;computational models&lt;/em&gt; for concurrent interaction:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;monitors, and shared memory concurrency generally&lt;/li&gt;
 &lt;li&gt;the actor model&lt;/li&gt;
 &lt;li&gt;channel-based communication&lt;/li&gt;
 &lt;li&gt;tuplespaces&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;These aren&amp;rsquo;t full programming languages, but there are many &lt;em&gt;programming models&lt;/em&gt; that build upon them. In some cases, development of these ideas has progressed all the way up to &lt;em&gt;system models&lt;/em&gt; including user interaction and so forth.&lt;/p&gt;

&lt;p&gt;The linked lecture notes include informal sketches of reduction semantics for each of the four models, plus a handful of small examples to give a feel for them.&lt;/p&gt;

&lt;p&gt;Lecture Notes:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/tree/master/conversational-context-and-concurrency/index.md"&gt;https://github.com/nuprl/hopl-s2017/tree/master/conversational-context-and-concurrency/index.md&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Discussion summary:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/blob/master/lecture_notes/2017-01-31.md"&gt;https://github.com/nuprl/hopl-s2017/blob/master/lecture_notes/2017&amp;ndash;01&amp;ndash;31.md&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description></item>
  <item>
   <title>Introducing HOPL 2017</title>
   <link>http://prl.ccs.neu.edu/blog/2017/02/15/introducing-hopl-2017/?utm_source=HOPL&amp;utm_medium=RSS</link>
   <guid>urn:http-prl-ccs-neu-edu:-blog-2017-02-15-introducing-hopl-2017</guid>
   <pubDate>Wed, 15 Feb 2017 01:21:37 UT</pubDate>
   <description>
&lt;p&gt;This semester at Northeastern, Matthias Felleisen is organizing the &lt;a href="http://www.ccs.neu.edu/home/matthias/7480-s17/index.html"&gt;History of Programming Languages&lt;/a&gt; seminar. Look for posts tagged &lt;code&gt;HOPL&lt;/code&gt; for updates from the lectures.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;Once every 6 to 8 years (i.e., once every batch of Ph.D. students?), &lt;a href="http://www.ccs.neu.edu/home/matthias"&gt;Matthias Felleisen&lt;/a&gt; teaches History of Programming Languages. Nominally, the course is a seminar. But unlike a typical seminar course, weekly topics are not the technical details from a handful of papers. Rather:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;The primary goal is to understand (some of) the discipline as it exists today and how some of its major themes evolved.&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;
 &lt;p&gt;The secondary goal is to develop basic skills for understanding and describing research themes. Every student will learn to study a theme via a series of papers, prepare an annotated bibliography, and present the key steps in the evolution of the theme.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Themes&lt;/strong&gt; is the operative word. To set the tone, this semester started with &amp;ldquo;themes that NUPRL faculty members have developed over the many decades of their careers.&amp;rdquo;&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Matthias, &lt;em&gt;Full Abstraction: From PCF to SPCF&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Jan Vitek, &lt;em&gt;From Encapsulation to Ownership&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Will Clinger, &lt;em&gt;Garbage Collection vs. Manual Allocation&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Olin Shivers, &lt;em&gt;Higher-order Flow Analysis&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Amal Ahmed, &lt;em&gt;Logical Relations: Stepping Beyond Toy Languages&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Matthias, &lt;em&gt;Programming Languages and Calculi&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Jan-Willem van de Meent, &lt;em&gt;Rescoring Strategies for Probabilistic Programs&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;(upcoming) Mitch Wand, &lt;em&gt;Analysis-Based Program Transformation&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;(upcoming) Frank Tip, &lt;em&gt;Refactoring&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;At this point in the course, we are just starting with the student presentations. As these presentations happen, we plan to push updates to this blog. All presentation materials are in the course repository:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017"&gt;https://github.com/nuprl/hopl-s2017&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Speakers&amp;rsquo; notes and annotated bibliographies are in top-level folders in the repo. Discussion summaries and &amp;ldquo;unofficial&amp;rdquo; notes are in the top-level &lt;a href="https://github.com/nuprl/hopl-s2017/tree/master/lecture_notes"&gt;&lt;code&gt;lecture_notes/&lt;/code&gt;&lt;/a&gt; folder.&lt;/p&gt;

&lt;p&gt;The list of upcoming presentations is online (along with &lt;a href="http://www.ccs.neu.edu/home/matthias/7480-s17/Summary___Materials.html"&gt;the papers&lt;/a&gt; each presentation is based on):&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="http://www.ccs.neu.edu/home/matthias/7480-s17/lectures.html"&gt;http://www.ccs.neu.edu/home/matthias/7480-s17/lectures.html&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Blogs posts for each talk should appear 2 weeks after the talk happens.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Links to past editions of HOPL:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="http://www.ccs.neu.edu/home/matthias/369-s10/index.html"&gt;Spring 2010&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://www.ccs.neu.edu/home/matthias/369-s04/index.html"&gt;Spring 2004&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description></item></channel></rss>