<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>PRL Blog: PRL Blog</title>
  <description>PRL Blog: PRL Blog</description>
  <link>http://prl.ccs.neu.edu/blog/index.html</link>
  <lastBuildDate>Wed, 15 Mar 2017 10:54:39 UT</lastBuildDate>
  <pubDate>Wed, 15 Mar 2017 10:54:39 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>Tracing JITs for Dynamic Languages</title>
   <link>http://prl.ccs.neu.edu/blog/2017/03/15/tracing-jits-for-dynamic-languages/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-03-15-tracing-jits-for-dynamic-languages</guid>
   <pubDate>Wed, 15 Mar 2017 10:54:39 UT</pubDate>
   <description>&lt;!-- more--&gt;

&lt;p&gt;Traditional JIT (just-in-time) compilers are method-based: they compile &amp;ldquo;hot&amp;rdquo; (i.e. frequently executed) methods to native code. An alternative is trace-based or tracing JITs, where the compilation unit is a (hot) sequence of instructions. Typically, such sequences of instructions correspond to loops, where programs spend most of their execution time.&lt;/p&gt;

&lt;p&gt;Where did the idea of tracing come from? What was appealing about it? How was tracing adapted for JITs and dynamic languages? What happened to Mozilla&amp;rsquo;s TraceMonkey, which used to be part of Firefox? Do any JITs today use tracing?&lt;/p&gt;

&lt;p&gt;In this talk, I trace tracing JITs from their origins to some of their recent developments. I cover five papers: the original tracing paper, an implementation of a tracing JIT for Java, the TraceMonkey JIT for JavaScript, PyPy&amp;rsquo;s &amp;ldquo;meta-level&amp;rdquo; tracing, and a specific class of optimizations for tracing JITs.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(The idea of using the phrase &amp;ldquo;trace tracing JITs&amp;rdquo; is from Matthias Felleisen.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;All materials can be found in the &lt;a href="https://github.com/nuprl/hopl-s2017/tree/master/tracing-jit"&gt;course repository&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/blob/master/tracing-jit/notes.pdf"&gt;Full notes&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/blob/master/tracing-jit/annotated.txt"&gt;Annotated bibliography&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description></item>
  <item>
   <title>Type Inference in Stack-Based Programming Languages</title>
   <link>http://prl.ccs.neu.edu/blog/2017/03/10/type-inference-in-stack-based-programming-languages/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-03-10-type-inference-in-stack-based-programming-languages</guid>
   <pubDate>Fri, 10 Mar 2017 16:23:30 UT</pubDate>
   <description>&lt;!-- more--&gt;

&lt;p&gt;Stack-based languages occupy a niche in today&amp;rsquo;s programming language environment. The predominant stack-based language in use by programmers is Forth, and is found mostly on embedded devices. These languages also find use as compile targets for more popular languages: the CIL and JVM are both stack-based. Less popular but highly interesting languages to mention include &lt;a href="http://www.kevinalbrecht.com/code/joy-mirror/joy.html"&gt;Joy&lt;/a&gt; and &lt;a href="http://factorcode.org/"&gt;Factor&lt;/a&gt;, known for their emphasis on higher-order stack-based programming.&lt;/p&gt;

&lt;p&gt;The majority of stack-based languages are not statically typed, and it would be a stretch to call Forth even dynamically typed. As such, developing large projects in Forth or Factor can require great discipline on the part of the programmer to avoid type errors.&lt;/p&gt;

&lt;p&gt;In this talk, I presented the development of type inference for stack-based languages as a linear sequence, divided into two overarching segments:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;An algebraic system known as &lt;em&gt;stack effects&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Systems that can be encoded as &lt;em&gt;nested pairs&lt;/em&gt; in standard functional  programming languages&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;The thread of research on stack effects began with Jaanus Pöial in the early 1990&amp;rsquo;s, and is a formalization of a commenting style well-known in the Forth community. The nested tuple systems were first examined by Okasaki in 1993 in the context of Haskell, and were later applied to higher-order stack-based languages. At the end, I give some avenues for extending the research on these systems, and list some pitfalls to be avoided in further research.&lt;/p&gt;

&lt;p&gt;Full notes (as PDF documents) &amp;mdash; see the &lt;a href="https://github.com/nuprl/hopl-s2017/tree/master/type-inference-for-stack-languages"&gt;git repository&lt;/a&gt; for more documents:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="/blog/static/stack-languages-talk-notes.pdf"&gt;Talk notes&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="/blog/static/stack-languages-annotated-bib.pdf"&gt;Annotated bibliography&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description></item>
  <item>
   <title>PLT Redex: mf-apply</title>
   <link>http://prl.ccs.neu.edu/blog/2017/03/03/plt-redex-mf-apply/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-03-03-plt-redex-mf-apply</guid>
   <pubDate>Fri, 03 Mar 2017 08:54:20 UT</pubDate>
   <description>
&lt;p&gt;The &lt;code&gt;mf-apply&lt;/code&gt; keyword is for checked metafunction application in PLT Redex. In other words, &lt;code&gt;(mf-apply f x)&lt;/code&gt; is just like &lt;code&gt;(f x)&lt;/code&gt;, but errors if &lt;code&gt;f&lt;/code&gt; is  not a previously-defined metafunction.&lt;/p&gt;

&lt;p&gt;Also, consider applying to attend &lt;em&gt;The Racket School of Semantics and Languages&lt;/em&gt; in Salt Lake City this summer: &lt;a href="http://summer-school.racket-lang.org/2017/"&gt;http://summer-school.racket-lang.org/2017/&lt;/a&gt;&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h2 id="metafunctions-vs-list-patterns"&gt;Metafunctions vs. List Patterns&lt;/h2&gt;

&lt;p&gt;Have you used PLT Redex? Good! Maybe this has happened to you:&lt;/p&gt;

&lt;div class="brush: racket"&gt;
 &lt;pre&gt;&lt;code&gt;#lang racket
(require redex)

;; -----------------------------------------------------------------------------
;; 1. You define a language
(define-language STLC
  [V ::= integer boolean C]
  [C ::= (closure Λ ρ)]
  [Λ ::= (λ (x : τ) M)]
  [M ::= (M M) V Λ x]
  [τ ::= Int Bool (τ → τ)]
  [ρ ::= ((x V) ...)]
  [Γ ::= ((x τ) ...)]
  [x ::= variable-not-otherwise-mentioned]
  #:binding-forms (λ (x : τ) M #:refers-to x))


;; -----------------------------------------------------------------------------
;; 2. You define a few metafunctions
(define-metafunction STLC
  closure-&amp;gt;lam : C -&amp;gt; Λ
  [(closure-&amp;gt;lam (closure Λ ρ))
   Λ])

(define-metafunction STLC
  closure-&amp;gt;env : C -&amp;gt; ρ
  [(closure-&amp;gt;env (closure Λ ρ))
   ρ])


;; -----------------------------------------------------------------------------
;; 3. You try defining a judgment form . . .
(define-judgment-form STLC
  #:mode (free-variables I O)
  #:contract (free-variables M (x ...))
  [
   --- FVS-Var
   (free-variables x (x))]
  [
   (free-variables M_0 (x_0 ...))
   (free-variables M_1 (x_1 ...))
   --- FVS-App
   (free-variables (M_0 M_1) (x_0 ... x_1 ...))]
  [
   (where (λ (x_0 τ) M) Λ)
   (free-variables M (x_1 ...))
   (where (x_2 ...) ,(set-remove (term (x_1 ...)) (term x_0)))
   --- FVS-Λ
   (free-variables Λ (x_2 ...))]
  [
   --- FVS-Integer
   (free-variables integer_0 ())]
  [
   --- FVS-Boolean
   (free-variables boolean_0 ())]
  [
   (where Λ (closure-&amp;gt;lam C))
   (free-variables Λ (x_0 ...))
   (where ((x_1 τ_1) ...) (closure-env C))
   (where (x_2 ...) ,(set-subtract (term (x_0 ...)) (term (x_1 ...))))
   --- FVS-Closure
   (free-variables C (x_2 ...))])


;; -----------------------------------------------------------------------------
;; 4. You test the judgment, and it mysteriously fails
(judgment-holds
  (free-variables (closure (λ (x : Int) x) ())
                  ()))
;; ==&amp;gt; #f&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;WHAT HAPPENED??!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The problem is this line in the &lt;code&gt;FVS-Closure&lt;/code&gt; rule:&lt;/p&gt;

&lt;div class="brush: racket"&gt;
 &lt;pre&gt;&lt;code&gt;   ....
   (where ((x_1 τ_1) ...) (closure-env C))
   ....&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;which checks that the list &lt;code&gt;(closure-env C)&lt;/code&gt; (whose first element is the  symbol &lt;code&gt;closure-env&lt;/code&gt; and second element is the symbol &lt;code&gt;C&lt;/code&gt;) matches the pattern  &lt;code&gt;((x_1 τ_1) ...)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Right.&lt;/p&gt;

&lt;p&gt;Of course you meant to apply the metafunction &lt;code&gt;closure-&amp;gt;env&lt;/code&gt; but made a typo. And since the syntax for metafunction application is the same as the syntax  for building a list, Redex doesn&amp;rsquo;t report an error.&lt;/p&gt;

&lt;p&gt;We can fix this code with the new &lt;a href="https://www.cs.utah.edu/plt/snapshots/current/doc/redex/The_Redex_Reference.html#%28form._%28%28lib._redex%2Freduction-semantics..rkt%29._mf-apply%29%29"&gt;&lt;code&gt;mf-apply&lt;/code&gt;&lt;/a&gt; keyword (available on &lt;a href="https://github.com/racket/racket"&gt;GitHub&lt;/a&gt; or in a &lt;a href="https://www.cs.utah.edu/plt/snapshots/"&gt;snapshot build&lt;/a&gt;):&lt;/p&gt;

&lt;div class="brush: racket"&gt;
 &lt;pre&gt;&lt;code&gt;   ....
   (where ((x_1 τ_1) ...) (mf-apply closure-env C))
   ....&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running &lt;code&gt;raco make&lt;/code&gt; now gives a compile-time error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  term: expected a previously defined metafunction
    at: closure-env
    in: (mf-apply closure-env C)&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id="but-i-still-need-to-type-mf-apply-correctly"&gt;But I still need to type &lt;code&gt;mf-apply&lt;/code&gt; correctly!&lt;/h3&gt;

&lt;p&gt;Leif Andersen says:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;I should point out that this has the issue of you still need to type &lt;code&gt;mf-apply&lt;/code&gt; correctly. ;)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That is, if you accidentally write:&lt;/p&gt;

&lt;div class="brush: racket"&gt;
 &lt;pre&gt;&lt;code&gt;   ....
   (where ((x_1 τ_1) ...) (mf-applu closure-env C))
   ....&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then the code compiles, thinking you intend to match a list of three elements  against the pattern.&lt;/p&gt;

&lt;p&gt;Never fear, there are at least two solutions.&lt;/p&gt;

&lt;h4 id="solution-1-rename-mf-apply"&gt;Solution 1: rename &lt;code&gt;mf-apply&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;A simple fix is to rename the &lt;code&gt;mf-apply&lt;/code&gt; keyword to something shorter (and harder to mis-type):&lt;/p&gt;

&lt;div class="brush: racket"&gt;
 &lt;pre&gt;&lt;code&gt;#lang racket
(require redex
         (rename-in redex
           [mf-apply MF]))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id="solution-2-the-mf-apply-lang-extension"&gt;Solution 2: the &lt;code&gt;mf-apply&lt;/code&gt; lang extension&lt;/h4&gt;

&lt;p&gt;A fancier solution is to install the &lt;code&gt;mf-apply&lt;/code&gt; meta-language.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ raco pkg install mf-apply&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This language updates the &lt;a href="http://docs.racket-lang.org/reference/readtables.html#%28tech._readtable%29"&gt;&lt;em&gt;readtable&lt;/em&gt;&lt;/a&gt;  to interpret S-expressions that begin with &lt;code&gt;#{&lt;/code&gt;:&lt;/p&gt;

&lt;div class="brush: racket"&gt;
 &lt;pre&gt;&lt;code&gt;#lang mf-apply racket
(require redex)

(term #{f x ...})&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;as a metafunction application:&lt;/p&gt;

&lt;div class="brush: racket"&gt;
 &lt;pre&gt;&lt;code&gt;#lang mf-apply racket
(require redex)

(term (mf-apply f x ...))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You the programmer only needs to write the &lt;code&gt;#{....}&lt;/code&gt; syntax.&lt;/p&gt;

&lt;p&gt;Source code is on GitHub:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/bennn/mf-apply"&gt;https://github.com/bennn/mf-apply&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;(It&amp;rsquo;s the simplest lang-extension I know of)&lt;/p&gt;

&lt;h2 id="what-is-plt-redex"&gt;What is PLT Redex?&lt;/h2&gt;

&lt;p&gt;PLT Redex is a library for semantics engineering. One of my favorite Redex features is it implements capture-avoiding substitution  and α-equivalence for any language with a &lt;code&gt;#:binding-forms&lt;/code&gt; specification  (such as STLC, above).&lt;/p&gt;

&lt;p&gt;You can read more:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;in the &amp;ldquo;Amb&amp;rdquo; tutorial: &lt;a href="http://docs.racket-lang.org/redex/tutorial.html"&gt;http://docs.racket-lang.org/redex/tutorial.html&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;in the &amp;ldquo;Long Tutorial&amp;rdquo;: &lt;a href="http://docs.racket-lang.org/redex/redex2015.html"&gt;http://docs.racket-lang.org/redex/redex2015.html&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;in the Redex reference manual: &lt;a href="http://docs.racket-lang.org/redex/The_Redex_Reference.html"&gt;http://docs.racket-lang.org/redex/The_Redex_Reference.html&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;on the PLT Redex website: &lt;a href="https://redex.racket-lang.org/"&gt;https://redex.racket-lang.org/&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;on GitHub: &lt;a href="https://github.com/racket/redex"&gt;https://github.com/racket/redex&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;And if you act now, you can become a &lt;em&gt;Redexan&lt;/em&gt; between July 10 and July 14  at the summer school in Salt Lake City, Utah:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="http://summer-school.racket-lang.org/2017/"&gt;http://summer-school.racket-lang.org/2017/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description></item>
  <item>
   <title>PLISS: Oregon without the greek</title>
   <link>http://prl.ccs.neu.edu/blog/2017/02/28/pliss-oregon-without-the-greek/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-02-28-pliss-oregon-without-the-greek</guid>
   <pubDate>Tue, 28 Feb 2017 23:01:00 UT</pubDate>
   <description>
&lt;p&gt;What does every student interested in programming languages need to learn about the practical side of the field? That is the question that the first international summer school on programming language implementation (or &lt;a href="https://pliss2017.github.io"&gt;PLISS&lt;/a&gt; for short) has set out to answer.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;&lt;img src="/img/pliss_summer_school_2017_logo.png" alt="PLISS logo" /&gt;&lt;/p&gt;

&lt;p&gt;The school will feature &lt;a href="https://pliss2017.github.io/speakers.html"&gt;twelve speakers&lt;/a&gt; versed in programming language practicae ranging from abstract interpretation to garbage collection and from compiler implementation to language design. The lectures will feature hands on exercises as well as lessons learned from large scale industrial efforts.&lt;/p&gt;

&lt;p&gt;Lectures cover current research and future trends in programming language design and implementation, including:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Writing Just-in-time Compilers with LLVM with Jan Vitek&lt;/li&gt;
 &lt;li&gt;Performance Evaluation and Benchmarking with Laurie Tratt&lt;/li&gt;
 &lt;li&gt;Designing a Commercial Actor Language with Sophia Drossopoulou and Heather Miller&lt;/li&gt;
 &lt;li&gt;High-Performance Fully Concurrent Garbage Collection with Richard Jones&lt;/li&gt;
 &lt;li&gt;Compiling Dynamic Languages with David Edelsohn&lt;/li&gt;
 &lt;li&gt;Language-support for Distributed Datastores with Suresh Jagannathan&lt;/li&gt;
 &lt;li&gt;Testing Programming Language Implementations with Alastair Donaldson&lt;/li&gt;
 &lt;li&gt;Abstract Interpretation and Static Analysis with lectures by Francesco Logozzo and Matt Might&lt;/li&gt;
 &lt;li&gt;The evolution of Scala with Martin Odersky&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;A summer school is also an opportunity to get know the speakers and get ideas for research problems. Students will have the opportunity to socialize with a peer group of other students in PL as well as professors and industrial researchers.&lt;/p&gt;

&lt;p&gt;Thanks to generous funding from NSF, ONR and SIGPLAN, costs will be kept low and some fellowships are available to cover travel costs.&lt;/p&gt;

&lt;p&gt;More information at:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://pliss2017.github.io"&gt;https://pliss2017.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Oregon is a reference to the &lt;a href="https://www.cs.uoregon.edu/research/summerschool/summer17/"&gt;OPLSS&lt;/a&gt;, in which you may also be interested.)&lt;/p&gt;</description></item>
  <item>
   <title>Linear Types for Low-level Languages</title>
   <link>http://prl.ccs.neu.edu/blog/2017/02/28/linear-types-for-low-level-languages/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-02-28-linear-types-for-low-level-languages</guid>
   <pubDate>Tue, 28 Feb 2017 09:51:55 UT</pubDate>
   <description>&lt;!-- more--&gt;

&lt;p&gt;In this talk, we covered early papers (primarily, by Girard, Lafont, and Abramsky) on linear logic and its reflections into computation. The goal was to understand why linearity is often turned to as a principled way to control resource usage, as shows up in a language like Rust. From the very beginning, researchers realized the implications for &amp;ldquo;low-level&amp;rdquo; languages - that linear resources would eliminate the need for garbage collection, allow in-place mutation, and enable safe parallel computation. However, pure implementations of linearity incur lots of copying, doing away with any efficiency gained, and we covered a survey of papers that attempted to reconcile this contradiction by weakening linearity in controlled ways.&lt;/p&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/blob/master/lecture_notes/2017-02-14.md"&gt;https://github.com/nuprl/hopl-s2017/blob/master/lecture_notes/2017&amp;ndash;02&amp;ndash;14.md&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Just after the talk, over lunch, we had a lab discussion about the phrase &amp;ldquo;low level&amp;rdquo;. Here are some thoughts:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;the phrase is relative, both over time and depending on the programming  task at hand&lt;/li&gt;
 &lt;li&gt;a &amp;ldquo;low level&amp;rdquo; task is &amp;ldquo;one that you shouldn&amp;rsquo;t need to worry about&amp;rdquo; while  solving your current task&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;And here are some example &amp;ldquo;low-level&amp;rdquo; tasks:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Time and space management is &amp;ldquo;low level&amp;rdquo; when designing a new algorithm  (the first question is correctness)&lt;/li&gt;
 &lt;li&gt;Calling conventions and endian-ness (facets of the damn machine running  the programs) are almost always low-level&lt;/li&gt;
 &lt;li&gt;Whether a given value is serializable is usually low-level&lt;/li&gt;
 &lt;li&gt;Possible side effects, thrown exceptions, and optional arguments can all  be considered &amp;ldquo;low level&amp;rdquo; aspects of library functions. This is low-level  in the sense that &amp;ldquo;I&amp;rsquo;d rather use a simpler type to think about this library&amp;rdquo;&lt;/li&gt;
 &lt;li&gt;Managing type annotations is a low-level detail in ML programs&lt;/li&gt;&lt;/ul&gt;</description></item>
  <item>
   <title>Bullets are good for your Coq proofs</title>
   <link>http://prl.ccs.neu.edu/blog/2017/02/21/bullets-are-good-for-your-coq-proofs/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-02-21-bullets-are-good-for-your-coq-proofs</guid>
   <pubDate>Tue, 21 Feb 2017 19:04:28 UT</pubDate>
   <description>
&lt;p&gt;I believe that bullets are one of the most impactful features of recent versions of Coq, among those that non-super-expert users can enjoy. They had a big impact on the maintainability of my proofs. Unfortunately, they are not very well-known, due to the fact that some introductory documents have not been updated to use them.&lt;/p&gt;

&lt;p&gt;Bullets are a very general construction and there are several possible ways to use them; I have iterated through different styles. In this post I will give the general rules, and explain my current usage style.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h2 id="why-bullets"&gt;Why bullets&lt;/h2&gt;

&lt;p&gt;While you are doing a proof, Coq shows a list of subgoals that have to be proved before the whole proof is complete. Most proof steps will operate on the current active subgoal, changing the hypotheses or the goal to prove, but some proof steps will split it into several subgoals (growing the total list of goals), or may terminate the proof of the current subgoal and show you the next active subgoal.&lt;/p&gt;

&lt;p&gt;Before bullets, a typical proof script would contain the proofs of each subgoal, one after another.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;induction foo. (* this creates many subgoal *)

proof of first subgoal.

proof of second subgoal.&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are many ways to structure this to make the structure more apparent: people would typically have a comment on each subgoal, or make disciplined use of indentation and blank lines. But, in my experience, a major problem with this style was maintainability in the face of changes to the definitions or parts of automation. It could be very hard of what was happening when a proof suddenly broke after a change before in the file:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;If a proof step now proves &lt;em&gt;less&lt;/em&gt; things, then what used to be the  end of a subgoal may not be anymore. Coq would then start reading  the proof of the next subgoal and try to apply it to the unfinished  previous goals, generating very confusing errors (you believe you  are in the second subgoal, but the context talks about a leaf case  of the first goal).&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;If a proof step now proves &lt;em&gt;more&lt;/em&gt; things, it is also very bad! The  next proof steps, meant for the first subgoal (for example), would  then apply to the beginning of the second subgoal, and you get very  confusing errors again.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;What we need for robustness is a way to indicate our &lt;em&gt;intent&lt;/em&gt; to Coq, when we think that a subgoal is finished and that a new subgoal starts, so that Coq can fail loudly at the moment where it notices that this intent does not match reality, instead of at an arbitrary later time.&lt;/p&gt;

&lt;p&gt;(The &lt;code&gt;S*Case&lt;/code&gt; tactics used in (older versions of) Software Foundations can solve this problem if used in a carefully, systematic way, and additionally provides naming. Alexandre Pilkiewicz implemented an even more powerful &lt;a href="https://github.com/pilki/cases"&gt;cases&lt;/a&gt; plugin. Bullets are available in standard Coq since 8.4 (released in 2012), and can be used with no effort.)&lt;/p&gt;

&lt;p&gt;There is not much discussion of bullets around; see the &lt;a href="https://coq.inria.fr/distrib/8.6/refman/Reference-Manual009.html#sec326"&gt;documentation&lt;/a&gt; in the Coq manual. I learned a lot from Arthur Azevedo de Amorim&amp;rsquo;s &lt;a href="https://github.com/arthuraa/poleiro/blob/master/theories/Bullets.v"&gt;Bullets.v&lt;/a&gt; file.&lt;/p&gt;

&lt;p&gt;Finally, some people don&amp;rsquo;t use bullets, because they systematically use so much automation that they never see subgoals &amp;mdash; each lemma has a one-line proof. This is also a valid style. (I have been going to Adam Chlipala&amp;rsquo;s &lt;a href="https://frap.csail.mit.edu/main"&gt;Formal Reasoning about Programs&lt;/a&gt; 2017 class, where Adam ignores bullets because that is his usual style.) Because I am not crushy enough to do this from the start, my proofs tend to start with cases and subgoals, and then I refine them to add more automation for robustness. I found bullets very useful for the first step, and during the refinement process.&lt;/p&gt;

&lt;h2 id="bullets"&gt;Bullets&lt;/h2&gt;

&lt;p&gt;Bullets are actually a combination of two features, braces &lt;code&gt;{ ... }&lt;/code&gt; and actual list bullets &amp;mdash; &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, or homogeneous repetitions of those, for example &lt;code&gt;--&lt;/code&gt; or &lt;code&gt;***&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id="braces"&gt;Braces&lt;/h3&gt;

&lt;p&gt;The opening brace &lt;code&gt;{&lt;/code&gt; focuses the proof on the current subgoal. If you finish the proof of the subgoal, the following subgoal will not become accessible automatically; you have to use the closing brace &lt;code&gt;}&lt;/code&gt; first. (If you finish the goal earlier than you think, you get an error.) Conversely, &lt;code&gt;}&lt;/code&gt; fails if the subgoal is not complete. (If you fail to finish, you get an error.)&lt;/p&gt;

&lt;p&gt;The previous example can thus be written as follows, and will be more robust:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;induction foo. (* this creates many subgoal *)
{
  proof of first subgoal.
}
{
  proof of second subgoal.
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you also want to make sure that an error occurs if the number of subgoals changes (for example if new constructors are added to the inductive type of &lt;code&gt;foo&lt;/code&gt;), you can use an outer layer of braces:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ induction foo. (* this creates many subgoal *)
  {
    proof of first subgoal.
  }
  {
    proof of second subgoal.
  }
} (* would fail if a new subgoal appeared *)&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id="list-bullets"&gt;List bullets&lt;/h3&gt;

&lt;p&gt;A bullet, for example &lt;code&gt;--&lt;/code&gt;, also focuses on the next subgoal. The difference is that when the subgoal is finished, you do not have a closing construction, you must use the same bullet to move to the next subgoal. (Again, this fails if the first proof step changes to prove too much or too little.) With bullets you would write&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;induction foo. (* this creates many subgoal *)
+ proof of first subgoal.
+ proof of second subgoal.&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bullets can be nested, but you must use different bullets for the different nesting levels. For example, if this proof is only one subgoal of a larger proof, you can use:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- induction foo. (* this creates many subgoal *)
  + proof of first subgoal.
  + proof of second subgoal.
- (* would fail if a new subgoal appeared *)
  rest of the proof&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The natural ordering of bullets, I think, is by increasing number of lines: &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;+&lt;/code&gt; then &lt;code&gt;*&lt;/code&gt; (and then multi-character bullets, I guess). You can also mix bullets with braces: the opening brace resets the bullet scope, any bullet can be used again with the subgoal.&lt;/p&gt;

&lt;p&gt;This gives a large space of freedom in how you want to use these features. You can use only braces, only bullets, braces and only one level of bullets, etc. My own style evolved with experience using the feature, and I will present the current status below.&lt;/p&gt;

&lt;h2 id="my-current-bullet-style"&gt;My current bullet style&lt;/h2&gt;

&lt;p&gt;When deciding how to use bullets, one distinguishes the commands that preserve the number of subgoals and those that may create new subgoals. I use some additional distinctions.&lt;/p&gt;

&lt;p&gt;Some tactics, for example &lt;code&gt;assert&lt;/code&gt;, create a number of subgoals that is &lt;em&gt;statically&lt;/em&gt; known, always the same for the tactic. I then use braces around each sub-proof, except the last one, which I think of as the &amp;ldquo;rest&amp;rdquo; of the current proof.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;assert foo as H.
{ proof of foo. }
rest of the proof using H:foo.&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(If the proof of &lt;code&gt;foo&lt;/code&gt; takes several lines, I two-indent them, with the braces alone on their lines.)&lt;/p&gt;

&lt;p&gt;Most tactics create a &lt;em&gt;dynamic&lt;/em&gt; number of subgoals, that depends on the specifics of the objects being operated on; this is the case of &lt;code&gt;case&lt;/code&gt;, &lt;code&gt;destruct&lt;/code&gt;, &lt;code&gt;induction&lt;/code&gt; for example. In this case, I open a brace before the tactic, and use a bullet for each subgoal.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ induction foo; simpl; auto.
- proof of first remaining subgoal.
- proof of second remaining subgoal.
  rest of the proof of the second subgoal.
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Notice that the subgoal-creating step is vertically aligned with the proof steps: I use both braces and bullets, but take only one indentation level each time.)&lt;/p&gt;

&lt;p&gt;As an exception, I may omit the braces if we are at the toplevel of the proof (&lt;code&gt;Proof .. Qed&lt;/code&gt; serve as braces).&lt;/p&gt;

&lt;p&gt;Note that omitting the braces here and using different bullets when you nest is also just fine. In my experience it gives proofs that are a bit more pleasant to read but also a bit more cumbersome to edit and move around.&lt;/p&gt;

&lt;p&gt;Finally, a not-uncommon mode of use of &amp;ldquo;dynamic&amp;rdquo; tactics in the sense above is to expect all the cases, except one, to be discharged by direct automation (for example they are all absurd except one). When it is my intent that all cases but one be discharged (and not a coincidence), I express it by not using braces (this command preserves the number of subgoals), but marking the remaining subgoal with a new bullet &lt;em&gt;without&lt;/em&gt; increasing the indentation level.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ induction foo.
- first subgoal.
- second subgoal.
  case blah; discharge all sub-subgoals but one.
+ remaining sub-subgoal of the second subgoal.
  finish the sub-subgoal.
- third subgoal.
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(This is the only time where I use several bullet levels.)&lt;/p&gt;

&lt;p&gt;If you are the kind of programmer that is passionate about indentation style, I should now have tricked you to use bullets to propose a different variant. Otherwise, please consider using bullets anyway, for example by following the style above, it will make your life easier in the face of changing developments.&lt;/p&gt;</description></item>
  <item>
   <title>Datalog for Static Analysis</title>
   <link>http://prl.ccs.neu.edu/blog/2017/02/21/datalog-for-static-analysis/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-02-21-datalog-for-static-analysis</guid>
   <pubDate>Tue, 21 Feb 2017 12:58:27 UT</pubDate>
   <description>&lt;!-- more--&gt;

&lt;p&gt;Datalog is an old DSL that frequently appears in work on static analysis. This edition of &lt;a href="/blog/2017/02/15/introducing-hopl-2017/"&gt;HOPL 2017&lt;/a&gt; explores the origins of Datalog in general, its early use in program analysis, and why Datalog remains a useful tool.&lt;/p&gt;

&lt;p&gt;Full notes:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/blob/master/datalog-for-static-analysis/datalog-for-static-analysis.pdf"&gt;https://github.com/nuprl/hopl-s2017/blob/master/datalog-for-static-analysis/datalog-for-static-analysis.pdf&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Datalog as a language was introduced by 1978 (its semantic foundations date back to 1976). It is &lt;em&gt;predicate logic&lt;/em&gt; as a database query language. The traditional view of a Datalog program is a &lt;em&gt;time invariant&lt;/em&gt; transformation over the &lt;em&gt;time varying&lt;/em&gt; data stored in an external database.&lt;/p&gt;

&lt;p&gt;In the early 1990&amp;rsquo;s, Uwe Aβmann designed a graph rewriting systems (EARS) that could:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Uniformly express various problems in static analysis&lt;/li&gt;
 &lt;li&gt;Systematically derive efficient solutions to such problems.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;(Prior work had derived the same solutions with ad-hoc methods.) Aβmann&amp;rsquo;s system is equivalent to Datalog.&lt;/p&gt;

&lt;p&gt;In 1993, Reps used the 
 &lt;tt&gt;CORAL&lt;/tt&gt; deductive database (an implementation of Datalog) to derive an on-demand (read: lazy) implementation of program slicing from a &lt;em&gt;specification&lt;/em&gt; of the slicing problem.&lt;/p&gt;

&lt;p&gt;Both Aβmann&amp;rsquo;s and Reps work appeared in 1994. This was the first time Datalog had been used to implement a static analysis.&lt;/p&gt;

&lt;p&gt;Researchers continue to use Datalog because:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;predicate logic (specifically: Horn clauses without function symbols or negation)  is useful for expressing recursive relations &amp;hellip; and static analyses are all about recursive relations&lt;/li&gt;
 &lt;li&gt;the language separates &lt;em&gt;specifications&lt;/em&gt; from their &lt;em&gt;implementation&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;there are many techniques for efficiently serving a Datalog query&lt;/li&gt;
 &lt;li&gt;these techniques have been implemented in &lt;a href="https://developer.logicblox.com/wp-content/uploads/2016/01/logicblox-sigmod15.pdf"&gt;at least one&lt;/a&gt;  commercial Datalog engine&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;For an excellent description of how Datalog can benefit static analysis, see the introduction to &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.648.1834&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Rep&amp;rsquo;s paper&lt;/a&gt;.&lt;/p&gt;</description></item>
  <item>
   <title>Conversational Context and Concurrency</title>
   <link>http://prl.ccs.neu.edu/blog/2017/02/15/conversational-context-and-concurrency/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-02-15-conversational-context-and-concurrency</guid>
   <pubDate>Wed, 15 Feb 2017 01:21:55 UT</pubDate>
   <description>&lt;!-- more--&gt;

&lt;p&gt;When programs are written with concurrency in mind, the programmer reasons about the interactions between concurrent components or agents in the program. This includes exchange of information, as well as management of resources, handling of partial failure, collective decision-making and so on.&lt;/p&gt;

&lt;p&gt;These components might be objects, or threads, or processes, or actors, or some more nebulous and loosely-defined concept; a group of callbacks, perhaps. The programmer has the notion of an agent in their mind, which translates into some representation of that agent in the program.&lt;/p&gt;

&lt;p&gt;We think about the contexts (because there can be more than one) in which agents exist in two different ways. From each agent&amp;rsquo;s perspective, the important thing to think about is the boundary between the agent and everything else in the system. But from the system perspective, we often think about &lt;em&gt;conversations&lt;/em&gt; between agents, whether it&amp;rsquo;s just two having an exchange, or a whole group collaborating on some task. Agents in a conversation play different roles, join and leave the group, and build shared conversational state.&lt;/p&gt;

&lt;p&gt;In this talk, I used the idea of these &lt;em&gt;conversational contexts&lt;/em&gt; as a lens through which to view the development of various metaphors and mechanisms of communication and coordination. I presented four &lt;em&gt;computational models&lt;/em&gt; for concurrent interaction:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;monitors, and shared memory concurrency generally&lt;/li&gt;
 &lt;li&gt;the actor model&lt;/li&gt;
 &lt;li&gt;channel-based communication&lt;/li&gt;
 &lt;li&gt;tuplespaces&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;These aren&amp;rsquo;t full programming languages, but there are many &lt;em&gt;programming models&lt;/em&gt; that build upon them. In some cases, development of these ideas has progressed all the way up to &lt;em&gt;system models&lt;/em&gt; including user interaction and so forth.&lt;/p&gt;

&lt;p&gt;The linked lecture notes include informal sketches of reduction semantics for each of the four models, plus a handful of small examples to give a feel for them.&lt;/p&gt;

&lt;p&gt;Lecture Notes:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/tree/master/conversational-context-and-concurrency/index.md"&gt;https://github.com/nuprl/hopl-s2017/tree/master/conversational-context-and-concurrency/index.md&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Discussion summary:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/blob/master/lecture_notes/2017-01-31.md"&gt;https://github.com/nuprl/hopl-s2017/blob/master/lecture_notes/2017&amp;ndash;01&amp;ndash;31.md&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description></item>
  <item>
   <title>Introducing HOPL 2017</title>
   <link>http://prl.ccs.neu.edu/blog/2017/02/15/introducing-hopl-2017/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-02-15-introducing-hopl-2017</guid>
   <pubDate>Wed, 15 Feb 2017 01:21:37 UT</pubDate>
   <description>
&lt;p&gt;This semester at Northeastern, Matthias Felleisen is organizing the &lt;a href="http://www.ccs.neu.edu/home/matthias/7480-s17/index.html"&gt;History of Programming Languages&lt;/a&gt; seminar. Look for posts tagged &lt;code&gt;HOPL&lt;/code&gt; for updates from the lectures.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;Once every 6 to 8 years (i.e., once every batch of Ph.D. students?), &lt;a href="http://www.ccs.neu.edu/home/matthias"&gt;Matthias Felleisen&lt;/a&gt; teaches History of Programming Languages. Nominally, the course is a seminar. But unlike a typical seminar course, weekly topics are not the technical details from a handful of papers. Rather:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;The primary goal is to understand (some of) the discipline as it exists today and how some of its major themes evolved.&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;
 &lt;p&gt;The secondary goal is to develop basic skills for understanding and describing research themes. Every student will learn to study a theme via a series of papers, prepare an annotated bibliography, and present the key steps in the evolution of the theme.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Themes&lt;/strong&gt; is the operative word. To set the tone, this semester started with &amp;ldquo;themes that NUPRL faculty members have developed over the many decades of their careers.&amp;rdquo;&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Matthias, &lt;em&gt;Full Abstraction: From PCF to SPCF&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Jan Vitek, &lt;em&gt;From Encapsulation to Ownership&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Will Clinger, &lt;em&gt;Garbage Collection vs. Manual Allocation&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Olin Shivers, &lt;em&gt;Higher-order Flow Analysis&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Amal Ahmed, &lt;em&gt;Logical Relations: Stepping Beyond Toy Languages&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Matthias, &lt;em&gt;Programming Languages and Calculi&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;Jan-Willem van de Meent, &lt;em&gt;Rescoring Strategies for Probabilistic Programs&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;(upcoming) Mitch Wand, &lt;em&gt;Analysis-Based Program Transformation&lt;/em&gt;&lt;/li&gt;
 &lt;li&gt;(upcoming) Frank Tip, &lt;em&gt;Refactoring&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;At this point in the course, we are just starting with the student presentations. As these presentations happen, we plan to push updates to this blog. All presentation materials are in the course repository:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017"&gt;https://github.com/nuprl/hopl-s2017&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Speakers&amp;rsquo; notes and annotated bibliographies are in top-level folders in the repo. Discussion summaries and &amp;ldquo;unofficial&amp;rdquo; notes are in the top-level &lt;a href="https://github.com/nuprl/hopl-s2017/tree/master/lecture_notes"&gt;&lt;code&gt;lecture_notes/&lt;/code&gt;&lt;/a&gt; folder.&lt;/p&gt;

&lt;p&gt;The list of upcoming presentations is online (along with &lt;a href="http://www.ccs.neu.edu/home/matthias/7480-s17/Summary___Materials.html"&gt;the papers&lt;/a&gt; each presentation is based on):&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="http://www.ccs.neu.edu/home/matthias/7480-s17/lectures.html"&gt;http://www.ccs.neu.edu/home/matthias/7480-s17/lectures.html&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Blogs posts for each talk should appear 2 weeks after the talk happens.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Links to past editions of HOPL:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="http://www.ccs.neu.edu/home/matthias/369-s10/index.html"&gt;Spring 2010&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://www.ccs.neu.edu/home/matthias/369-s04/index.html"&gt;Spring 2004&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description></item>
  <item>
   <title>[Toward Type-Preserving Compilation of Coq, at POPL17 SRC (cross-post)](https://williamjbowman.com/blog/2017/01/03/toward-type-preserving-compilation-of-coq-at-popl17-src/)</title>
   <link>http://prl.ccs.neu.edu/blog/2017/01/03/-toward-type-preserving-compilation-of-coq-at-popl17-src-cross-post-https-williamjbowman-com-blog-2017-01-03-toward-type-preserving-compilation-of-coq-at-popl17-src/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-01-03-toward-type-preserving-compilation-of-coq-at-popl17-src-cross-post-https-williamjbowman-com-blog-2017-01-03-toward-type-preserving-compilation-of-coq-at-popl17-src</guid>
   <pubDate>Tue, 03 Jan 2017 15:15:57 UT</pubDate>
   <description></description></item>
  <item>
   <title>Fall 2016 PL Junior Retrospective</title>
   <link>http://prl.ccs.neu.edu/blog/2017/01/02/fall-2016-pl-junior-retrospective/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-01-02-fall-2016-pl-junior-retrospective</guid>
   <pubDate>Mon, 02 Jan 2017 16:39:37 UT</pubDate>
   <description>
&lt;p&gt;The &lt;a href="http://prl.ccs.neu.edu/seminars.html"&gt;Programming Language Seminar, Junior&lt;/a&gt; (or “PL Junior”), is a seminar for junior students to learn and discuss topics at a pace more suitable to our background. This semester, we decided to study dependent types. We chose this topic because&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;working from the &lt;a href="https://mitpress.mit.edu/books/types-and-programming-languages"&gt;TAPL&lt;/a&gt; presentation of type systems, dependent types are a step up in difficulty (excepting F-omega-sub), and&lt;/li&gt;
 &lt;li&gt;they represent a significant increase in the reasoning power of types over programs.&lt;/li&gt;&lt;/ol&gt;
&lt;!-- more--&gt;

&lt;p&gt;There was a preference for learning how to implement a dependent type system, instead of spending a significant amount of time reading papers, especially dense type-theoretic papers suggested by &lt;a href="http://purelytheoretical.com/sywtltt.html"&gt;posts&lt;/a&gt; like &lt;a href="http://jozefg.bitbucket.org/posts/2015-08-14-learn-tt.html"&gt;these&lt;/a&gt;. So we followed the &lt;a href="https://github.com/sweirich/pi-forall"&gt;pi-for-all&lt;/a&gt; lecture series given by Stephanie Weirich at &lt;a href="https://www.cis.upenn.edu/~bcpierce/attapl/"&gt;OPLSS&lt;/a&gt;, which focuses on implementing a simple dependently-typed programming language.&lt;/p&gt;

&lt;p&gt;After the pi-for-all lectures, we read chapter two of Edwin Brady’s &lt;a href="https://eb.host.cs.st-andrews.ac.uk/writings/thesis.pdf"&gt;dissertation on implementing dependently typed languages&lt;/a&gt;. The thesis includes a relatively approachable introduction to TT, the core dependent type theory of Epigram.&lt;/p&gt;

&lt;p&gt;Along the way, we became sidetracked by &lt;a href="https://en.wikipedia.org/wiki/System_U#Girard.27s_paradox"&gt;Girard’s paradox&lt;/a&gt;. In the first pi-for-all lecture, we came across the Type-in-Type rule. (In a dependent type system the term and the type languages are the same. However, we still need to distinguish what is a “program” and what is a “type,” for instance, so that we can determine that the annotation of a function’s argument is valid. So a construct in the term language is Type, which is meant to describe those things that are valid in programs where we expect to find a type). In the lecture, this prompted the comment that this (“of course”) makes our system inconsistent as a logic, but there was no further elaboration, and we could not figure out how to use this fact to show inconsistency.&lt;/p&gt;

&lt;p&gt;It turns out the reason Type-in-Type is inconsistent is quite complicated. It is explained in a &lt;a href="https://www.cs.cmu.edu/~kw/scans/hurkens95tlca.pdf"&gt;paper&lt;/a&gt; that we had difficulty understanding. So we turned to the students in our lab that have expertise in the area. The answer we received is that, intuitively, it is inconsistent for the same reason as Russell’s paradox (or the Burali-Forti paradox), but the actual proof is actually quite involved. The lesson we drew is that despite being “obvious,” Type-in-Type being inconsistent is not easy to prove. The way people seem to throw around this conclusion is confusing from a beginner’s point of view.&lt;/p&gt;

&lt;p&gt;The best thing about the pi-for-all series is that it demystified dependent types for us. We gained confidence in being able to whiteboard a dependent type system with the ease of System-F or STLC. If we had one complaint, the presentation of the material relied heavily on Haskell details. The unbound library to handle variables in the implementation results in a somewhat “magicy” representation of binding; it’s not clear that the benefits are so great as to outweigh the cost of just implementing alpha-equivalence and capture-avoiding-substitution. Overall they were high-quality lectures. As hinted above, we didn’t particularly care for the second lecture that was mostly a code walk-through. One advantage of watching videos was that we could speed through parts we were already comfortable with.&lt;/p&gt;

&lt;p&gt;With Edwin Brady’s dissertation, we got a glimpse of how quickly the details of a dependently typed language get hairy. Looking at you, inductive data definitions and eliminations. There were some extremely large type signatures. While this exercise boosted our confidence that we could read Serious Dependent Types™ papers, it also gave evidence that our fears of incomprehensibility were not completely unfounded.&lt;/p&gt;

&lt;p&gt;This issue appeared before in our discussion of Girard’s Paradox. In the discussion of the paradox, we got stuck when we tried to understand the very complex term that inhabited the bottom type. Dependent typing, and discussions thereof, allow very rich, meaningful, and complex types that are as complex as the code that they abstract over. While we are used to understanding these structures in code, parsing a complex type and its fundamental meaning gave us considerable difficulty.&lt;/p&gt;

&lt;h2 id="thoughts-on-the-format"&gt;Thoughts on the format&lt;/h2&gt;

&lt;p&gt;Our meetings this semester were all of the form “we’ll watch this lecture or read this chapter, and then discuss it next week.” Next semester we would like to go back to presenting each week. We feel doing presentations forces the presenter to reach a deeper understanding of the material. This semester we got a relatively shallow understanding of a broad area. A deeper understanding with a narrower focus may be more beneficial (or complementary).&lt;/p&gt;

&lt;p&gt;[[Sam’s defense as Grand Convener of PL Junior: Once we picked dependent types as a topic, doing presentations was not an option. We didn’t have the expertise in the area to pick out different sub-topics and papers suitable for presentations. And, since we were short-handed (4 people each week), we would be presenting once a month!]]&lt;/p&gt;

&lt;p&gt;If we continue learning more about dependent types it would be by: 1) Doing more reading, such as the &lt;a href="https://www.cis.upenn.edu/~bcpierce/attapl/"&gt;ATTAPL&lt;/a&gt; chapter or the programming in Martin-Löf’s type theory material. 2) Actually trying to implement some of the things we’ve learned this semester 3) Playing around with more of the various dependent type systems and theorem provers out there&lt;/p&gt;

&lt;p&gt;For future pl junior cohorts or anyone else in learning about dependent types: Pi-for-all is useful material, but could be condensed into two weeks (for example, by watching the lectures at 1.5x speed) instead of four. Don’t worry about Type-in-Type. The Epigram material is OK but you might be better served looking at ATTAPL first. At some point, you will have to read the dense papers, but pi-for-all is a good introduction.&lt;/p&gt;</description></item>
  <item>
   <title>Measuring the submission/review balance</title>
   <link>http://prl.ccs.neu.edu/blog/2016/12/17/measuring-the-submission-review-balance/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2016-12-17-measuring-the-submission-review-balance</guid>
   <pubDate>Sat, 17 Dec 2016 16:33:10 UT</pubDate>
   <description>
&lt;p&gt;How do researchers know whether they are doing &amp;ldquo;enough&amp;rdquo; or &amp;ldquo;too many&amp;rdquo; reviews? A measurable goal is to be review-neutral: to have demanded, through our submissions, as many reviews as we have produced as reviewers.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="reviewing-is-good"&gt;Reviewing is good&lt;/h3&gt;

&lt;p&gt;I like to review academic papers. It is a very rewarding activity in many different ways. One gets to serve the academic community, helping it function smoothly. One gets a chance at acquiring a much better understanding of someone else&amp;rsquo;s work than idle paper-skimming allows. One gets to send feedback to our colleagues and help them improve their work and its presentation &amp;mdash; it is also an essential way in which we can participate to the formation of student researchers all over the world. Finally, doing reviews helped me develop the skill the judge someone else&amp;rsquo;s work and of forcing oneself to come up with a decisive opinion &amp;mdash; it is surprisingly difficult and only comes with training.&lt;/p&gt;

&lt;p&gt;Doing reviews is also fairly time-consuming. I noticed that the time I spend on each review is generally stable (excursions into previous or related work excluded): around one day and a half for conference reviews, and at least twice more for journal reviews &amp;mdash; I&amp;rsquo;m sure other people have wildly different figures, but I would expect it to be a noticeable time commitment in any case. (Workshop reviews are much easier, at least for the formats I have seen of 2-page extended abstracts, I&amp;rsquo;d say one hour per review.)&lt;/p&gt;

&lt;h3 id="how-many-reviews"&gt;How many reviews?&lt;/h3&gt;

&lt;p&gt;Because it is so time-consuming, deciding whether to say &amp;ldquo;yes&amp;rdquo; or &amp;ldquo;no&amp;rdquo; to invitations to review a new paper is not easy: in general I want to say &amp;ldquo;yes&amp;rdquo; (unless I can tell that I will not enjoy reading the paper at all), but it is not reasonable to say &amp;ldquo;yes&amp;rdquo; all the time, because I also need to spend time on other things. When should I say &amp;ldquo;no&amp;rdquo; because I have done &amp;ldquo;too many&amp;rdquo; reviews already?&lt;/p&gt;

&lt;p&gt;We can count the number of reviews that we have done, and we can also estimate the number of reviews that we have demanded of others through our submissions. A natural goal for researchers is to produce at least as many reviews as they demand; if everyone reached this goal, the peer-review system would be at equilibrium without imposing too much of a workload on anyone.&lt;/p&gt;

&lt;p&gt;To estimate the number of reviews a researcher demanded from their peers, you can sum, for each of their submissions to a peer-reviewed venue, the number of reviews that they received, divided by the total number of authors of the submissions.&lt;/p&gt;

&lt;p&gt;
 &lt;script type="math/tex; mode=display"&gt; \sum_{p \in \mathtt{Submissions}} \frac{\mathtt{reviews}(p)}{\mathtt{authors}(p)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Out of curiosity, I just measured this balance for myself: over my years doing research I have &amp;ldquo;demanded&amp;rdquo; 10 workshop reviews and 28.5 conference reviews, and &amp;ldquo;produced&amp;rdquo; 6 workshop reviews and 17 conference reviews. If you think that an article would interest me, you shouldn&amp;rsquo;t feel bad about asking me to review it, for now. (On the other hand, my balance &lt;em&gt;this year&lt;/em&gt; is positive, so I wouldn&amp;rsquo;t feel to bad about refusing if I had to.)&lt;/p&gt;

&lt;p&gt;Of course, a researcher&amp;rsquo;s balance is highly dependent on where they are in their academic career &amp;mdash; maybe more so that on their personal choices. Students are supposed to submit articles, but are offered few opportunities for doing reviews. When they are invited to do reviews, it is often as sub-reviewer, one review at a time. More established researchers participate in program committees, where they have to do a fair amount of reviews at once &amp;mdash; ten to twenty can be typical in Programming Languages conferences. This means that one naturally starts with a deficit of reviews, and that the opportunity to become balanced or positive only comes over the years.&lt;/p&gt;

&lt;p&gt;(There is much more that could be said about the dynamics of the submission/review balance. I think the idea that a single person should be neutral should not be taken too seriously, because the dynamics are so complex. For example, some people stop doing reviews with a negative balance (students going to the industry for example), so long-time researchers necessarily have a &lt;em&gt;very positive&lt;/em&gt; balance that may make short-time researchers balance considerations mostly irrelevant. Another thing is that there is no point doing more reviews than required by the submission flow, and that doing more reviews would build up more reviewing debt under this neutrality criterion &amp;mdash; you can never have everyone positive.)&lt;/p&gt;

&lt;h3 id="quality"&gt;Quality&lt;/h3&gt;

&lt;p&gt;This is only a comment on the quantitative aspects of reviewing. Much more important is the qualitative part: are the reviews you receive and produce good reviews? (There is no objective definition of what a good review is; I like reviews that are constructive, help improve the work and its presentation, and catch mistakes.) For a given paper, one or a few very good reviews is more helpful than many bad reviews, so one should not compromise on the quality of one&amp;rsquo;s reviews in order to reach a quantitative goal.&lt;/p&gt;

&lt;h3 id="advice-for-students"&gt;Advice for students?&lt;/h3&gt;

&lt;p&gt;While proof-reading this post (thanks!), Ben asked some questions that may be of interest to others &amp;mdash; mostly students, I suppose.&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;If I want to be review-neutral, but I have to accumulate a &amp;ldquo;review debt&amp;rdquo; before I can start reviewing, does this mean I should accept my first opportunity to review and every one that follows (until I&amp;rsquo;m neutral)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The answer is of course &amp;ldquo;no&amp;rdquo;: one should never feel forced to accept reviews. On the other hand, I do think that it is worthwhile for PhD students to take advantage of the reviews they are offered, so &amp;ldquo;saying yes most of the time&amp;rdquo; sounds like a reasonable strategy to me &amp;mdash; this is just a personal opinion. Some reasons:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;Reviewing is hard and takes training, I think it is good to start  practicing early. Students are in a good situation to exercise their  reviewing skills at a fairly calm peace (you won&amp;rsquo;t get many  reviews anyway), and with more time than more senior people.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Student reviews are often done as sub-reviewer: someone does  a review, but also asks for your opinion and includes your  sub-review in their review. It is a low-pressure way to do your  first reviews, and the ability to exchange opinions with the other  reviewer and discuss both reviews is really helpful. Students can  also ask for feedback on their reviews to their advisor, which is  also very helpful.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Reviewing teaches a few useful things about writing papers as  well &amp;mdash; it&amp;rsquo;s always easier to recognize the flaws in others&amp;rsquo; work.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;On the other hand, I think you should not accept reviews at times when you cannot invest enough work in the review, or when doing so would be detrimental to you &amp;mdash; whether you are on a deadline, or under too much pressure, or have conflicting commitments, etc. This is more important than anything about a submission/review balance.&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;Do you have any ideas for how young researchers / new researchers can reduce their &amp;ldquo;review footprint&amp;rdquo;? For example, is it possible to volunteer for reviews?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, you can volunteer for reviews by telling the colleagues in your lab that you would be interested in doing reviews and that they should consider giving you some.&lt;/p&gt;

&lt;p&gt;(With the increased use of double-blind submission processes, it is becoming more difficult to pass conference reviews to external researchers. This means that students are relatively unlikely to receive review offers from outside their close colleagues.)&lt;/p&gt;

&lt;p&gt;Besides doing more reviews, the two other factors one could in theory play with are: submitting less papers, and having more co-authors. I think there is something to be said for the first one: one reason to not submit unfinished, buggy or topically-inappropriate articles is that it has a review cost. The second factor should not be considered, I think: &amp;ldquo;did this person contribute to the work?&amp;rdquo; should weight infinitely more for co-authorship decisions.&lt;/p&gt;

&lt;p&gt;Note: Another thing you can ask for is &lt;em&gt;reading reviews other people received&lt;/em&gt;. I think that reading reviews is also very helpful for research beginners &amp;mdash; whether reviews of one&amp;rsquo;s own work or someone else&amp;rsquo;s. In particular, I wouldn&amp;rsquo;t know how to write reviews if I hadn&amp;rsquo;t had the opportunity to read reviews before that. If someone you are close to receives reviews, you should consider asking them whether you could have a look.&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;Is being a student volunteer at a conference equal to &amp;ldquo;one review&amp;rdquo;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it is a distinct form of academic service. I don&amp;rsquo;t know how to measure the &amp;ldquo;conference organization cost&amp;rdquo; we impose to our academic colleagues. (If there are around 500 attendants to a typical Programming Languages conference, it means that for every 500 conferences you attend you should organize one all by yourself.)&lt;/p&gt;</description></item>
  <item>
   <title>[Getting Started in Programming Languages (Cross-Post)](http://jschuster.org/blog/2016/11/29/getting-started-in-programming-languages/)</title>
   <link>http://prl.ccs.neu.edu/blog/2016/11/30/-getting-started-in-programming-languages-cross-post-http-jschuster-org-blog-2016-11-29-getting-started-in-programming-languages/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2016-11-30-getting-started-in-programming-languages-cross-post-http-jschuster-org-blog-2016-11-29-getting-started-in-programming-languages</guid>
   <pubDate>Wed, 30 Nov 2016 15:24:45 UT</pubDate>
   <description></description></item>
  <item>
   <title>SRC-submissions</title>
   <link>http://prl.ccs.neu.edu/blog/2016/11/17/src-submissions/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2016-11-17-src-submissions</guid>
   <pubDate>Thu, 17 Nov 2016 13:52:52 UT</pubDate>
   <description>
&lt;p&gt;Max New, Daniel Patterson and Ben Greenman recently wrote three two-page abstracts on what they are working on right now. Come have a look &amp;mdash; and any feedback is welcome!&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h2 id="gradual-type-precision-as-retraction"&gt;Gradual Type Precision as Retraction&lt;/h2&gt;

&lt;p&gt;&lt;a href="http://maxsnew.github.io/docs/precision-as-retraction.pdf"&gt;Gradual Type Precision as Retraction&lt;/a&gt;
 &lt;br /&gt;Max New
 &lt;br /&gt;2016&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;Gradually typed programming languages allow for a mix of precision of static type information, allowing advanced type features to be added to existing languages, while still supporting interoperability with legacy code. The advantages of gradual typing are enticing to researchers and practitioners alike, but a general theory of gradually typed languages is only beginning to emerge after a decade of research.&lt;/p&gt;
 &lt;p&gt;It has long been noted that there is much similarity between work on contracts and gradual typing, and the use of retracts in domain theory which were used to relate models of untyped and typed lambda calculus in &lt;a href="https://pdfs.semanticscholar.org/359e/ca57fe42d97cbb67f0b5591869abe5eb5421.pdf"&gt;Scott(1976)&lt;/a&gt; and &lt;a href="http://andrewkish-name.s3.amazonaws.com/scott80.pdf"&gt;Scott(1980)&lt;/a&gt;. Here we take this connection seriously and consider how judgments in modern gradually typed languages can be framed in terms of retractions. While retractions in programming languages were originally studied in terms of denotational semantics in domains, our presentation will use only the most basic elements of category theory: composition, identity and equality of terms, so our formulation is equally applicable to axiomatic or operational semantics.&lt;/p&gt;
 &lt;p&gt;In particular we propose a semantic criterion for the notion of precision of gradual types, a common judgment in gradually typed languages (sometimes called naive subtyping for historical reasons). We relate it to a previous definition from &lt;a href="https://www.eecs.northwestern.edu/%7Erobby/pubs/papers/esop2009-wf.pdf"&gt;Wadler and Findler(2009)&lt;/a&gt; that defines type precision in terms of blame. We show that our definition decomposes in a similar way into “positive” and “negative” type precision, but without depending on a specific notion of blame in the language.&lt;/p&gt;&lt;/blockquote&gt;

&lt;h2 id="linking-types-specifying-safe-interoperability-and-equivalences"&gt;Linking Types: Specifying Safe Interoperability and Equivalences&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://dbp.io/pubs/2016/linking-types-poplsrc2017-proposal.pdf"&gt;Linking Types: Specifying Safe Interoperability and Equivalences&lt;/a&gt;
 &lt;br /&gt;Daniel Patterson
 &lt;br /&gt;2016&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;All programs written in high-level languages link with libraries written in lower-level languages, often to expose constructs, like threads, random numbers, or automatic serialization, that aren’t possible in the high-level language. This linking usually takes place after compiling both languages to a common language, possibly assembly. In this sense, reasoning about crosslanguage linking means reasoning about compilation.&lt;/p&gt;
 &lt;p&gt;While most languages include cross-language linking (FFI) mechanisms, they are ad-hoc and can easily break the semantic equivalences of the source language, making it hard for source programmers to reason about correctness of their programs and hard for compiler writers to reason about correctness of their optimizations.&lt;/p&gt;
 &lt;p&gt;In this work, I design and motivate linking types, a language-based mechanism for formally specifying safe linking with libraries utilizing features inexpressible in the source. Linking types allows programmers to reason about their programs in the presence of behavior inexpressible in their language, without dealing with the intricacies of either the compiler or the particular language they are linking with.&lt;/p&gt;&lt;/blockquote&gt;

&lt;h2 id="pruning-contracts-with-rosette"&gt;Pruning Contracts with Rosette&lt;/h2&gt;

&lt;p&gt;&lt;a href="http://www.ccs.neu.edu/home/types/resources/popl2017-src.pdf"&gt;Pruning Contracts with Rosette&lt;/a&gt;
 &lt;br /&gt;Ben Greenman
 &lt;br /&gt;2016&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;&lt;a href="http://www.ccs.neu.edu/racket/pubs/icfp16-dnff.pdf"&gt;Contracts&lt;/a&gt; are a pragmatic tool for managing software systems, but programs using contracts suffer runtime overhead. If this overhead becomes a performance bottleneck, programmers must manually edit or remove their contracts. This is no good. Rather, the contracts should identify their own inefficiencies and remove unnecessary dynamic checks. Implementing contracts with &lt;a href="https://emina.github.io/rosette/"&gt;Rosette&lt;/a&gt; is a promising way to build such self-aware contracts.&lt;/p&gt;&lt;/blockquote&gt;

&lt;h2 id="while-were-at-it-lets-rant-on-srcs"&gt;While we&amp;rsquo;re at it let&amp;rsquo;s rant on SRCs&lt;/h2&gt;

&lt;p&gt;These abstracts are submitted at POPL&amp;rsquo;s &amp;ldquo;Student Research Competition&amp;rdquo;. You submit an abstract, and if you get accepted to that thing, you get a bit of travel support money, and you have to prepare a poster and present it at the conference.&lt;/p&gt;

&lt;p&gt;I have a firm dislike for the &lt;em&gt;Competition&lt;/em&gt; part of that concept: I think that people think of research too competitively already, and that we should have less of that, not more. (Having some is unfortunately unavoidable in scarce-resource situations.) I think that the process of awarding prizes to students with the &amp;ldquo;best poster&amp;rdquo; is dumb &amp;mdash; and borderline ridiculous.&lt;/p&gt;

&lt;p&gt;On the other hand, my experience seeing them writing these extended abstracts is that it&amp;rsquo;s a useful exercise for them, and produces nice result &amp;mdash; short, readable introductions to their ideas. And Jennifer Paykin &lt;a href="https://github.com/gasche/icfp2016-blog/blob/master/SVs/jennifer_paykin.md"&gt;convincingly argues&lt;/a&gt; that although writing a poster is rather painful, actually presenting it during the conference is interesting and useful. In her words, &amp;ldquo;it&amp;rsquo;s worth it to get the experience of authentic and fruitful discussions&amp;rdquo;. Plus having posters in the corridors of one&amp;rsquo;s lab is very nice.&lt;/p&gt;

&lt;p&gt;I think we could have &amp;ldquo;Student Research Sessions&amp;rdquo; or &amp;ldquo;Student Poster Sessions&amp;rdquo;, where students are encouraged to present their work, would write those nice extended abstracts and posters, interact with researchers at the conference, and get travel money, without the ranking and prize stuff. (I would still encourage students to participate to SRC today, it seems to be worth it.)&lt;/p&gt;</description></item>
  <item>
   <title>Understanding Constructive Galois Connections</title>
   <link>http://prl.ccs.neu.edu/blog/2016/11/16/understanding-constructive-galois-connections/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2016-11-16-understanding-constructive-galois-connections</guid>
   <pubDate>Wed, 16 Nov 2016 00:00:00 UT</pubDate>
   <description>
&lt;p&gt;One of my favorite papers at ICFP 2016 (in lovely &lt;a href="http://conf.researchr.org/home/icfp-2016"&gt;Nara, Japan&lt;/a&gt;) was &lt;a href="https://arxiv.org/abs/1511.06965"&gt;Constructive Galois Connections: Taming the Galois Connection Framework for Mechanized Metatheory&lt;/a&gt; by &lt;a href="http://david.darais.com/"&gt;David Darais&lt;/a&gt; and &lt;a href="https://www.cs.umd.edu/~dvanhorn/"&gt;David Van Horn&lt;/a&gt;. The central technical result is quite interesting, but a little intimidating, so I&amp;rsquo;d like to share a &amp;ldquo;de-generalization&amp;rdquo; of the result that I found helpful to understand.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h1 id="history"&gt;History&lt;/h1&gt;

&lt;p&gt;I won&amp;rsquo;t go into much of the details of the paper, because I think it is quite well written, but here&amp;rsquo;s a short overview. The paper is about how to do verified static analysis while taking advantage of the calculational approach of &lt;a href="http://www.di.ens.fr/~cousot/COUSOTpapers/Marktoberdorf98.shtml"&gt;Abstract Interpretation&lt;/a&gt;. The problem is that the Galois connections people use for abstract domains are not always computable. Darais and Van Horn show however that there is a very useful class of Galois connections that is computable, and they show how they can exploit this to write verified static analyses that more closely follow the &amp;ldquo;on-paper&amp;rdquo; proofs, and offload much of the details to the proof assistant as mere calculation.&lt;/p&gt;

&lt;p&gt;David Darais told me about these results when we were at POPL 2016 (in less lovely but much more convenient &lt;a href="http://conf.researchr.org/home/POPL-2016"&gt;St. Petersburg, Florida&lt;/a&gt;) and in particular about the central theorem of the paper, which shows that two different classes of Galois connections they define, &amp;ldquo;Kleisli&amp;rdquo; and &amp;ldquo;Constructive&amp;rdquo; Galois connections, are actually constructively equivalent. I was really surprised by the result when he explained it to me, and so I hoped to find if there was a known generalization of the result for adjunctions of categories, rather than Galois connections of posets.&lt;/p&gt;

&lt;p&gt;Eventually, my usual trawling of &lt;a href="http://mathoverflow.net/"&gt;Mathoverflow&lt;/a&gt; and &lt;a href="https://ncatlab.org/nlab/show/HomePage"&gt;nlab&lt;/a&gt; led me to a &lt;a href="https://ncatlab.org/nlab/show/Cauchy+complete+category#InOrdinaryCatTheoryByProfunctors"&gt;not-quite generalization to categories&lt;/a&gt; and interestingly a &lt;a href="http://mathoverflow.net/questions/222516/duality-between-compactness-and-hausdorffness/222524#222524"&gt;&lt;em&gt;de&lt;/em&gt;-generalization to sets&lt;/a&gt; that helped me immensely to understand the theorem.&lt;/p&gt;

&lt;p&gt;Since I know that the original theorem is a bit technical, I&amp;rsquo;ll explain the de-generalization to sets here, which I hope will help to understand their theorem.&lt;/p&gt;

&lt;h1 id="functions-and-relations"&gt;Functions and Relations&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s start with the &amp;ldquo;Kleisli Arrows&amp;rdquo;, which are monotone functions 
 &lt;script type="math/tex"&gt;f : A \to P(B)&lt;/script&gt; where 
 &lt;script type="math/tex"&gt;A,B&lt;/script&gt; are posets and 
 &lt;script type="math/tex"&gt;P(B)&lt;/script&gt; represents the poset of downward-closed subsets of 
 &lt;script type="math/tex"&gt;B&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now to &amp;ldquo;de-posetize&amp;rdquo; this, we&amp;rsquo;ll take sets 
 &lt;script type="math/tex"&gt;X,Y&lt;/script&gt; and let 
 &lt;script type="math/tex"&gt;P(Y)&lt;/script&gt; mean the powerset of 
 &lt;script type="math/tex"&gt;Y&lt;/script&gt;, that is the set of all subsets of 
 &lt;script type="math/tex"&gt;Y&lt;/script&gt;. Then a function 
 &lt;script type="math/tex"&gt;f : X \to P(Y)&lt;/script&gt; is actually exactly the same thing as a relation 
 &lt;script type="math/tex"&gt;R \subset X \times Y&lt;/script&gt;. From 
 &lt;script type="math/tex"&gt;f :
X \to P(Y)&lt;/script&gt; we can take 
 &lt;script type="math/tex"&gt;R = \{(x,y) \in X\times Y | y\in f(x)\}&lt;/script&gt; and from 
 &lt;script type="math/tex"&gt;R&lt;/script&gt; we can construct 
 &lt;script type="math/tex"&gt;f(x) = \{y \in Y | (x,y) \in R \}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Furthermore, the &amp;ldquo;Kleisli composition&amp;rdquo; is the same as composition of relations. If 
 &lt;script type="math/tex"&gt;R \subset X \times Y&lt;/script&gt; and 
 &lt;script type="math/tex"&gt;Q \subset Y \times Z&lt;/script&gt;, then the composition is defined as 
 &lt;script type="math/tex; mode=display"&gt; (R;Q) = \{(x,z) \in X \times Z | \exists y\in Y. (x,y) \in R \land (y,z) \in Q\}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Then the next thing we need to understand is what is the de-generalization of &amp;ldquo;Kleisli Galois connection&amp;rdquo;? Well, Galois connections are an instance of what&amp;rsquo;s called an adjunction in category theory, which is usually formulated in terms of categories, functors and natural transformations. However, you can interpret the definition of adjunction in any &amp;ldquo;universe&amp;rdquo; that acts like the universe of categories, functors and natural transformations and it turns out we have such a universe. The universe I&amp;rsquo;m talking about is called 
 &lt;script type="math/tex"&gt;\texttt{Rel}&lt;/script&gt;, and it consists of sets, relations between sets and &lt;em&gt;inclusion of relations&lt;/em&gt;, i.e. that one relation is a subset of another.&lt;/p&gt;

&lt;p&gt;Then what does it mean to have an adjunction between two relations 
 &lt;script type="math/tex"&gt;R \subset X \times Y, Q \subset Y \times X&lt;/script&gt;? Taking apart the definition it just means&lt;/p&gt;

&lt;p&gt;\begin{align}\tag{1}  \Delta(X) \subset R;Q \end{align} \begin{align}\tag{2}  Q;R \subset \Delta(Y) \end{align}&lt;/p&gt;

&lt;p&gt;where 
 &lt;script type="math/tex"&gt;\Delta&lt;/script&gt; means the &lt;em&gt;diagonal&lt;/em&gt;, or equality relation on the set:&lt;/p&gt;

&lt;p&gt;
 &lt;script type="math/tex; mode=display"&gt;\Delta(X) = \{(x_1,x_2) \in X | x_1 = x_2 \}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So we just need to unravel what (1) and (2) mean above. Unwinding (1), we get that for any 
 &lt;script type="math/tex"&gt;x \in X&lt;/script&gt;, there exists a 
 &lt;script type="math/tex"&gt;y \in Y&lt;/script&gt; such that 
 &lt;script type="math/tex"&gt;(x,y) \in R&lt;/script&gt; and 
 &lt;script type="math/tex"&gt;(y,x) \in Q&lt;/script&gt;. This tells us for one that 
 &lt;script type="math/tex"&gt;R&lt;/script&gt; is a &amp;ldquo;right-total&amp;rdquo; relation and 
 &lt;script type="math/tex"&gt;Q&lt;/script&gt; is a &amp;ldquo;left-total&amp;rdquo; relation. Every 
 &lt;script type="math/tex"&gt;x&lt;/script&gt; is related to some 
 &lt;script type="math/tex"&gt; y&lt;/script&gt; by 
 &lt;script type="math/tex"&gt; R&lt;/script&gt; and 
 &lt;script type="math/tex"&gt; Q&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;If we unwind (2), we get that for any 
 &lt;script type="math/tex"&gt;y,y' \in Y&lt;/script&gt; if there&amp;rsquo;s some 
 &lt;script type="math/tex"&gt;x \in X&lt;/script&gt; such that 
 &lt;script type="math/tex"&gt;(x,y) \in R&lt;/script&gt; and 
 &lt;script type="math/tex"&gt;(y',x) \in Q&lt;/script&gt; then actually 
 &lt;script type="math/tex"&gt;y = y')&lt;/script&gt;. This one is a bit more mysterious, but first, let&amp;rsquo;s see what this tells us about the relationship between 
 &lt;script type="math/tex"&gt;R&lt;/script&gt; and 
 &lt;script type="math/tex"&gt;Q&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;If 
 &lt;script type="math/tex"&gt;(x,y) \in R&lt;/script&gt;, then by (1) there&amp;rsquo;s some 
 &lt;script type="math/tex"&gt;y' \in Y&lt;/script&gt; so that 
 &lt;script type="math/tex"&gt;(x,y') \in R&lt;/script&gt; and 
 &lt;script type="math/tex"&gt;(y',x) \in Q&lt;/script&gt;. Then, by (2) we know that 
 &lt;script type="math/tex"&gt;y = y'&lt;/script&gt;, so we&amp;rsquo;ve shown that if 
 &lt;script type="math/tex"&gt;(x,y) \in R&lt;/script&gt; then 
 &lt;script type="math/tex"&gt;(y,x)
\in Q&lt;/script&gt;. Then a completely symmetric argument shows that if 
 &lt;script type="math/tex"&gt;(y,x)
\in Q&lt;/script&gt; then 
 &lt;script type="math/tex"&gt;(x,y)\in R&lt;/script&gt;! So we&amp;rsquo;ve discovered that actually 
 &lt;script type="math/tex"&gt;Q&lt;/script&gt; is just the opposite relation of 
 &lt;script type="math/tex"&gt;R&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Then if we look at (2) again but replace the 
 &lt;script type="math/tex"&gt;Q&lt;/script&gt;&amp;rsquo;s by flipped 
 &lt;script type="math/tex"&gt;R&lt;/script&gt;&amp;rsquo;s we get that for any 
 &lt;script type="math/tex"&gt;y,y' \in Y&lt;/script&gt;, if there&amp;rsquo;s some 
 &lt;script type="math/tex"&gt;x
\in X&lt;/script&gt; such that 
 &lt;script type="math/tex"&gt;(x,y) \in R&lt;/script&gt; and 
 &lt;script type="math/tex"&gt;(x,y')\in R&lt;/script&gt; then 
 &lt;script type="math/tex"&gt;y
= y'&lt;/script&gt;, which tells us that 
 &lt;script type="math/tex"&gt;R&lt;/script&gt; is a partial function, i.e., that every 
 &lt;script type="math/tex"&gt;x&lt;/script&gt; is related to at most one 
 &lt;script type="math/tex"&gt;y&lt;/script&gt; by 
 &lt;script type="math/tex"&gt;R&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;You may recognize it now, our 
 &lt;script type="math/tex"&gt;R \subset X \times Y&lt;/script&gt; is just a function, and saying 
 &lt;script type="math/tex"&gt;R, Q&lt;/script&gt; are adjoint is exactly the same as saying that 
 &lt;script type="math/tex"&gt;Q = R^{\text{op}}&lt;/script&gt; and 
 &lt;script type="math/tex"&gt;R&lt;/script&gt; is a function. Adjunctions are so pervasive you saw them back in pre-algebra!&lt;/p&gt;

&lt;h1 id="constructive-galois-connections"&gt;Constructive Galois Connections&lt;/h1&gt;

&lt;p&gt;Back to constructive Galois connections, I hope if you read the paper you can see that their theorem is a generalization of the above argument, where instead of relations we have &amp;ldquo;monotone relations&amp;rdquo;, i.e., downward-closed 
 &lt;script type="math/tex"&gt;R \subset A^{\text{op}} \times B&lt;/script&gt;. Then you can interpret the definition of adjunction in that universe and get that it&amp;rsquo;s the same as a Kleisli Galois connection and that a similar argument to the above shows that the &amp;ldquo;left adjoint&amp;rdquo; is represented by a monotone function 
 &lt;script type="math/tex"&gt;f : A \to B&lt;/script&gt;:&lt;/p&gt;

&lt;p&gt;
 &lt;script type="math/tex; mode=display"&gt;R = \{(x,y) | y \le f(x) \}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Which shows that every Kleisli Galois connection is actually a constructive Galois connection! The details are in their paper, and I hope they are easier to follow now.&lt;/p&gt;

&lt;p&gt;In fact, we get a little extra from what&amp;rsquo;s mentioned in their paper, which is that the &amp;ldquo;right adjoint&amp;rdquo; is represented by 
 &lt;script type="math/tex"&gt;f&lt;/script&gt; as well but in the opposite way:&lt;/p&gt;

&lt;p&gt;
 &lt;script type="math/tex; mode=display"&gt;Q = \{(y,x) | f(x) \le y \}&lt;/script&gt;&lt;/p&gt;

&lt;h1 id="category-theory-post-scriptum"&gt;Category Theory Post Scriptum&lt;/h1&gt;

&lt;p&gt;If you&amp;rsquo;re interested in Category theory, here&amp;rsquo;s a more technical addendum.&lt;/p&gt;

&lt;p&gt;Remembering from Category Theory class, sets are just posets where objects are only less than themselves and posets are (basically) categories where there is at most 1 arrow between objects, so we might naturally ask, does this theorem extend to categories?&lt;/p&gt;

&lt;p&gt;Well, first we need a generalization from relations to downward-closed relations to what are called &lt;a href="https://ncatlab.org/nlab/show/profunctor"&gt;distributors or profunctors&lt;/a&gt;. Then we can also generalize inclusion of relations to morphisms of distributors and ask, is every left adjoint distributor represented by a functor?&lt;/p&gt;

&lt;p&gt;The answer is, at least in full generality, no! For it to be true we need a special property on the codomain of the left adjoint 
 &lt;script type="math/tex"&gt;R : C
\not\to D&lt;/script&gt;, which is called (for mind-boggling reasons) &lt;a href="https://ncatlab.org/nlab/show/Cauchy+complete+category#InOrdinaryCatTheoryByProfunctors"&gt;Cauchy completeness&lt;/a&gt;. Viewing sets and posets as special categories, it turns out that they always have this property, and that&amp;rsquo;s why the theorem worked out for those adjunctions.&lt;/p&gt;</description></item>
  <item>
   <title>Beta Reduction (Part 1)</title>
   <link>http://prl.ccs.neu.edu/blog/2016/11/02/beta-reduction-part-1/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2016-11-02-beta-reduction-part-1</guid>
   <pubDate>Wed, 02 Nov 2016 21:10:18 UT</pubDate>
   <description>
&lt;p&gt;The λ-calculus is often introduced by showing how to build a real programming language from it&amp;rsquo;s simple syntactic forms. In this series of post, I attempt to introduce it as a tool for modeling semantics. So if you&amp;rsquo;re opening &lt;a href="https://www.amazon.com/Calculus-Semantics-Studies-Foundations-Mathematics/dp/0444875085"&gt;Barendregt&lt;/a&gt; for the first time, trying to understand a lecture from a programming languages or functional programming class, or just starting to become involved in PL research, I hope this post will help you understand evaluation by substitution (β-reduction).&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This post is aimed at myself around 18 months ago. At the time, I had spent a fair amount of time programming, taken a functional programming class, and been introduced to the λ-calculus. Despite that, I didn&amp;rsquo;t really understand how the λ-calculus was really used in PL research and how it really worked. When I was asked to prove a novel theorem about β-reduction, I really struggled. I spent a lot of time looking online for an explanation of the proofs I could understand. This series of posts is my attempt to rectify that.&lt;/p&gt;

&lt;p&gt;This first post briefly introduces the λ-calculus and explains β-reduction through a formally defined semantics and examples in &lt;a href="https://racket-lang.org/"&gt;Racket&lt;/a&gt;, &lt;a href="http://ocaml.org/"&gt;OCaml&lt;/a&gt;, and &lt;a href="https://www.haskell.org/"&gt;Haskell&lt;/a&gt;. My goal in this post is to develop an intuition about what β-reduction is and how it works. In a followup post, I&amp;rsquo;ll explain how to prove that β-reduction is confluent.&lt;/p&gt;

&lt;h1 id="the-λ-calculus"&gt;The λ-Calculus&lt;/h1&gt;

&lt;p&gt;The λ-calculus is a simple model of computation developed by &lt;a href="https://en.wikipedia.org/wiki/Alonzo_Church"&gt;Alonzo Church&lt;/a&gt;. The λ-calculus has three different syntactic forms: variables, anonymous functions (lambdas), and function application. The &lt;a href="http://matt.might.net/articles/grammars-bnf-ebnf/"&gt;BNF&lt;/a&gt; for the λ-calculus is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;e ::= x
   | λx.e
   | e e&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the above BNF, &lt;code&gt;x&lt;/code&gt; is a metavariable, standing for any variable. In this post, I use &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, &lt;code&gt;z&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;, and &lt;code&gt;b&lt;/code&gt; as variables in my examples. The &lt;code&gt;λx.e&lt;/code&gt; term represents a function with a single parameter &lt;code&gt;x&lt;/code&gt;. We say that the parameter, &lt;code&gt;x&lt;/code&gt; is bound in &lt;code&gt;e&lt;/code&gt;. It is possible to have unbound variables in the λ-calculus, though for the most part, we can ignore them in our discussion of this topic. Finally, applications consist of two expressions. The first expression is the function and the second is its argument. Parenthesis are often added for clarity.&lt;/p&gt;

&lt;p&gt;If you program regularly in a functional language, this might look fairly familiar to you. In fact, you can compute anything in the λ-calculus that you can in any other language. In other words, the λ-calculus is Turing complete. Of course, you wouldn&amp;rsquo;t want to program in this language as it&amp;rsquo;s significantly harder to encode some of these constructs than just using built in language constructs like numbers. I&amp;rsquo;m not going to discuss these ideas in detail, but if you&amp;rsquo;re interested in how to add numbers, booleans, conditionals, and recursion to the λ-calculus, Matt Might has a few great posts about how to do this using equivalent constructs in &lt;a href="http://matt.might.net/articles/python-church-y-combinator/"&gt;Python&lt;/a&gt;, &lt;a href="http://matt.might.net/articles/church-encodings-demo-in-scheme/"&gt;Scheme&lt;/a&gt;, and &lt;a href="http://matt.might.net/articles/js-church/"&gt;JavaScript&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now that we&amp;rsquo;ve discussed the syntax of the language, we can look at the semantics, or how terms evaluate. Below I have the evaluation rules for the λ-calculus. The arrow represents β-reduction which is the subject of this post. The semantics rely on substitution which is depicted with brackets. I have also defined the substitution function. I&amp;rsquo;m assuming the Barendregt &lt;a href="http://www.cse.chalmers.se/research/group/logic/TypesSS05/Extra/geuvers.pdf"&gt;variable convention (2.6)&lt;/a&gt; which states that every bound variable is distinct from every free variable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x[ x := e ] = e
y[ x := e ] = y
(λx.e1)[ x := e2 ] = (λx.e1[ x := e2 ])
(e1 e2)[ x := e3 ] = (e1[ x := e3 ] e2[ x := e3 ])&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the substitution function defined, we can write a semantics for evaluation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;------------------------------
  (λx.e1) e2 -&amp;gt;β e1[ x := e2 ]

    e1 -&amp;gt;β e1'
--------------------
  e1 e2 -&amp;gt;β e1' e2

    e2 -&amp;gt;β e2
--------------------
  e1 e2 -&amp;gt;β e1 e2'&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These rules mean that if you have a term in which you have a function applied to an argument, you substitute the argument into the function. The next two rules say that if you can apply an evaluation rule to either the first or second subterm, you may do so. Notice that both the second and third rules might apply to a single term. These rules are nondeterministic and in these cases it is fine to use whichever rule you want (see below).&lt;/p&gt;

&lt;h1 id="what-is-β-reduction"&gt;What is β-reduction?&lt;/h1&gt;

&lt;p&gt;More generally, what is reduction? Reduction is a model for computation that consists of a set of rules that determine how a term is stepped forwards. β-reduction is reduction by function application. When you β-reduce, you remove the λ from the function and substitute the argument for the function&amp;rsquo;s parameter in its body. More formally, we can define β-reduction as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(λx.e1) e2 = e1[ x := e2 ]&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Given that definition, lets look at a few examples. I&amp;rsquo;ve written the examples in the λ-calculus, Racket, OCaml, and Haskell. It&amp;rsquo;s important to note that these languages have more restricted semantics than the original λ-calculus. These restrictions are called reduction strategies. In Racket and OCaml, it is not possible to substitute with anything except for a value, meaning you need to evaluate the argument until it is a function before substituting. This makes the evaluation reduce left to right before substituting. This restriction is called &amp;ldquo;call by value&amp;rdquo;. In Haskell, no reduction occurs in arguments, meaning that we would omit the third rule. This could potentially require an expression to be evaluated multiple times. In reality, Haskell caches the results of these evaluations so each expression is only evaluated once, but I&amp;rsquo;m going to ignore that here. This presentation of Haskell&amp;rsquo;s semantics is called &amp;ldquo;call by name&amp;rdquo; and the optimization is called &amp;ldquo;call by need&amp;rdquo;. (For more details on lazy evaluation, I recommend: &lt;a href="http://www.ccs.neu.edu/racket/pubs/esop12-cf.pdf"&gt;Chang and Felleisen, ESOP 2012&lt;/a&gt;.)&lt;/p&gt;

&lt;h1 id="some-examples"&gt;Some Examples&lt;/h1&gt;

&lt;p&gt;The first example evaluates the same way in all of the languages.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    (λx.x) (λy.y)
-&amp;gt;β x[ x := (λy.y) ]
=   (λy.y)&lt;/code&gt;&lt;/pre&gt;

&lt;div class="brush: racket"&gt;
 &lt;pre&gt;&lt;code&gt;    ((λ (x) x) (λ (y) y))
-&amp;gt;β x[ x := (λ (y) y) ]
=   (λ (y) y)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="brush: ocaml"&gt;
 &lt;pre&gt;&lt;code&gt;    (fun x -&amp;gt; x) (fun y -&amp;gt; y)
-&amp;gt;β x[x := (fun y -&amp;gt; y) ]
=   x[x := (fun y -&amp;gt; y) ]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="brush: haskell"&gt;
 &lt;pre&gt;&lt;code&gt;    (\x -&amp;gt; x) (\y -&amp;gt; y)
-&amp;gt;β x[ x := (\y -&amp;gt; y) ]
=   (\y -&amp;gt; y)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In our next example, we will see the differences between the languages. They all reduce to the same thing, albeit in different ways.&lt;/p&gt;

&lt;div class="brush: haskell"&gt;
 &lt;pre&gt;&lt;code&gt;    (\x -&amp;gt; \y -&amp;gt; y) ((\z -&amp;gt; z) (\a -&amp;gt; a))
-&amp;gt;β (\y -&amp;gt; y)[x := ((\z -&amp;gt; z) (\a -&amp;gt; a))]
= (\y -&amp;gt; y)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that the application on the right hand side is never evaluated. In an eager language like Racket or OCaml, the term being substituted must be a value, so the evaluation follows a different path.&lt;/p&gt;

&lt;div class="brush: racket"&gt;
 &lt;pre&gt;&lt;code&gt;    ((λ (x) (λ (y) y)) ((λ (z) z) (λ (a) a)))
-&amp;gt;β ((λ (x) (λ (y) y)) (z[ z := (λ (a) a) ]))
=   ((λ (x) (λ (y) y)) (λ (a) a))
-&amp;gt;β (λ (y) y)[ x := (λ (a) a) ]
=   (λ (y) y)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="brush: ocaml"&gt;
 &lt;pre&gt;&lt;code&gt;    (fun x -&amp;gt; (fun y -&amp;gt; y)) ((fun z -&amp;gt; z) (fun a -&amp;gt; a))
-&amp;gt;β (fun x -&amp;gt; (fun y -&amp;gt; y)) (z[ z := (fun a -&amp;gt; a) ])
=   (fun x -&amp;gt; (fun y -&amp;gt; y)) (fun a -&amp;gt; a)
-&amp;gt;β (fun y -&amp;gt; y)[ x := (fun a -&amp;gt; a) ]
=   (fun y -&amp;gt; y)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The nondeterministic semantics of the λ-calculus lead to two possible paths through the above computation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    (λx.λy.y) ((λz.z) (λa.a))
-&amp;gt;β (λx.λy.y) (z[ z := (λa.a) ])
=   (λx.λy.y) (λa.a)
-&amp;gt;β (λy.y)[ x := (λa.a) ]
=   (λy.y)&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;    (λx.λy.y) ((λz.z) (λa.a))
-&amp;gt;β (λy.y)[ x := ((λz.z) (λa.a)) ]
=   (λy.y)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets look at a final example. This one is more complicated than the previous ones. I&amp;rsquo;m also going to stop showing the substitution explicitly. In the literature, it is fairly common not to see it explicitly, but you should still be able to follow what&amp;rsquo;s going on.&lt;/p&gt;

&lt;div class="brush: racket"&gt;
 &lt;pre&gt;&lt;code&gt;    ((λ (x) x) (((λ (a) (λ (b) (a b))) (λ (y) y)) (λ (z) z)))
-&amp;gt;β ((λ (x) x) ((λ (b) ((λ (y) y) b)) (λ (z) z)))
-&amp;gt;β ((λ (x) x) ((λ (y) y) (λ (z) z)))
-&amp;gt;β ((λ (x) x) (λ (z) z))
-&amp;gt;β (λ (z) z)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The same thing happens in OCaml&lt;/p&gt;

&lt;div class="brush: ocaml"&gt;
 &lt;pre&gt;&lt;code&gt;    (fun x -&amp;gt; x) ((fun a -&amp;gt; (fun b -&amp;gt; a b)) (fun y -&amp;gt; y) (fun z -&amp;gt; z));;
-&amp;gt;β (fun x -&amp;gt; x) ((fun b -&amp;gt; (fun y -&amp;gt; y) b) (fun z -&amp;gt; z));;
-&amp;gt;β (fun x -&amp;gt; x) ((fun y -&amp;gt; y) (fun z -&amp;gt; z))
-&amp;gt;β (fun x -&amp;gt; x) (fun z -&amp;gt; z)
-&amp;gt;β (fun z -&amp;gt; z)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In Haskell, the situation is a little different:&lt;/p&gt;

&lt;div class="brush: haskell"&gt;
 &lt;pre&gt;&lt;code&gt;    (\x -&amp;gt; x) ((\a -&amp;gt; \b -&amp;gt; a b) (\y -&amp;gt; y) (\z -&amp;gt; z))
-&amp;gt;β (\a -&amp;gt; \b -&amp;gt; a b) (\y -&amp;gt; y) (\z -&amp;gt; z)
-&amp;gt;β (\b -&amp;gt;  (\y -&amp;gt; y) b) (\z -&amp;gt; z)
-&amp;gt;β (\y -&amp;gt; y) (\z -&amp;gt; z)
-&amp;gt;β (\z -&amp;gt; z)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, in the λ-calculus, things can go a few different ways.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    (λx.x) ((λa.λb.a b) (λy.y) (λz.z))
-&amp;gt;β (λx.x) ((λb.(λy.y) b) (λz.z))
-&amp;gt;β (λx.x) ((λy.y) (λz.z))
-&amp;gt;β (λy.y) (λz.z)
-&amp;gt;β (λz.z)&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;    (λx.x) ((λa.λb.a b) (λy.y) (λz.z))
-&amp;gt;β (λa.λb.a b) (λy.y) (λz.z)
-&amp;gt;β (λb.(λy.y) b) (λz.z)
-&amp;gt;β (λy.y) (λz.z)
-&amp;gt;β (λz.z)&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;    (λx.x) ((λa.λb.a b) (λy.y) (λz.z))
-&amp;gt;β (λx.x) ((λb.(λy.y) b) (λz.z))
-&amp;gt;β (λb.(λy.y) b) (λz.z)
-&amp;gt;β (λy.y) (λz.z)
-&amp;gt;β (λz.z)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;rsquo;s a very interesting property of all of those examples: they all evaluate to the same thing, regardless of the reduction order taken. This is because β-reduction of the λ-calculus has the weak Church-Rosser Property. In systems that have this property, if a reduction sequence terminates, it will always evaluate to the same term, regardless of the path taken. This property does not necessarily hold if the term does not terminate. See if you can work out what happens to this term with each reduction strategy:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(λx.λy.y) ((λa.a a) (λb.b b))&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A weaker property is called confluence, which states that all reduction sequences can be stepped to a common term. At the beginning of this post, I said that the λ-calculus is used as a model for real programming languages. This is generally true. There are proofs that the λ-calculus has this property, but people don&amp;rsquo;t tend to prove such things about their actual languages, it is usually enough to build them knowing that their theoretical model has the property. In a follow up post, I&amp;rsquo;ll explain the proofs that the λ-calculus does indeed have these properties.&lt;/p&gt;

&lt;p&gt;P.S. In Racket, you can look at the examples using the &lt;a href="http://docs.racket-lang.org/stepper/"&gt;stepper&lt;/a&gt; which will allow you to interactively run the term and see how it reduces.&lt;/p&gt;</description></item>
  <item>
   <title>Meaningful Distinctions</title>
   <link>http://prl.ccs.neu.edu/blog/2016/10/31/meaningful-distinctions/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2016-10-31-meaningful-distinctions</guid>
   <pubDate>Mon, 31 Oct 2016 17:20:33 UT</pubDate>
   <description>
&lt;blockquote&gt;
 &lt;p&gt;&amp;ldquo;Meaningful distinctions deserve to be maintained.&amp;rdquo; &amp;mdash; Errett A. Bishop&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Likewise, memorable quotations deserve to be read in context. In this spirit, I am happy to present the above &amp;ldquo;basic principle&amp;rdquo; in its context:  &lt;a href="/img/sicm.pdf"&gt;&lt;em&gt;Schizophrenia in contemporary mathematics&lt;/em&gt; (pdf)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Read on for a brief summary.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;hr /&gt;

&lt;p&gt;I first read the above quotation in &lt;a href="http://www.michaelbeeson.com/research/papers/BishopForeword.pdf"&gt;Michael Beeson&amp;rsquo;s introduction&lt;/a&gt; to the 2012 edition of Bishop&amp;rsquo;s &lt;a href="https://www.amazon.com/Foundations-Constructive-Analysis-Errett-Bishop/dp/4871877140"&gt;&lt;em&gt;Foundations of Constructive Analysis&lt;/em&gt;&lt;/a&gt;. That was two years ago.&lt;/p&gt;

&lt;p&gt;Last month, I tried to find its context. &lt;a href="https://books.google.com/books?id=uPx8tGCaxzUC&amp;amp;pg=PA214&amp;amp;lpg=PA214&amp;amp;dq=meaningful+distinctions+deserve+to+be+maintained&amp;amp;source=bl&amp;amp;ots=cWjwOTnNuT&amp;amp;sig=wN143wNyfXtMFLGABBQM-22aSOQ&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwjPwt2HmYbQAhWE6IMKHU5rB8YQ6AEIHjAA#v=onepage&amp;amp;q=meaningful%20distinctions%20deserve%20to%20be%20maintained&amp;amp;f=false"&gt;Many&lt;/a&gt; &lt;a href="https://www.jstor.org/stable/2589553"&gt;other&lt;/a&gt; &lt;a href="https://books.google.com/books?id=J4DkBwAAQBAJ&amp;amp;pg=PA6&amp;amp;lpg=PA6&amp;amp;dq=meaningful+distinctions+deserve+to+be+maintained&amp;amp;source=bl&amp;amp;ots=KYkrkBrJd_&amp;amp;sig=AAK1A_uIkQlVcYCY1TFljfA3CqA&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwjPwt2HmYbQAhWE6IMKHU5rB8YQ6AEIJTAC#v=onepage&amp;amp;q=meaningful%20distinctions%20deserve%20to%20be%20maintained&amp;amp;f=false"&gt;uses&lt;/a&gt; &lt;a href="https://books.google.com/books?id=oN5nsPkXhhsC&amp;amp;pg=PR6&amp;amp;lpg=PR6&amp;amp;dq=meaningful+distinctions+deserve+to+be+maintained&amp;amp;source=bl&amp;amp;ots=4doTufVdsy&amp;amp;sig=u3e_Z_xdN-tjt9p1eqQ88juA5Ns&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwjPwt2HmYbQAhWE6IMKHU5rB8YQ6AEIKDAD#v=onepage&amp;amp;q=meaningful%20distinctions%20deserve%20to%20be%20maintained&amp;amp;f=false"&gt;of&lt;/a&gt; &lt;a href="https://books.google.com/books?id=GR44SKXCZJsC&amp;amp;pg=RA1-PA199&amp;amp;lpg=RA1-PA199&amp;amp;dq=meaningful+distinctions+deserve+to+be+maintained&amp;amp;source=bl&amp;amp;ots=lNpzR5QV7h&amp;amp;sig=IGg2Q_KtreSAhrbSJxsV7mQ8xok&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwjPwt2HmYbQAhWE6IMKHU5rB8YQ6AEIMDAF#v=onepage&amp;amp;q=meaningful%20distinctions%20deserve%20to%20be%20maintained&amp;amp;f=false"&gt;the&lt;/a&gt; &lt;a href="http://www.math.canterbury.ac.nz/php/groups/cm/faq/"&gt;quote&lt;/a&gt; &lt;a href="http://www.ben-sherman.net/aux/evident-logic.pdf"&gt;cited&lt;/a&gt; a &lt;em&gt;Schizophrenia in comtemporary mathematics&lt;/em&gt;, but I could not find an electronic copy. (It turns out, the AMS Bookstore page for &lt;a href="http://bookstore.ams.org/conm-39"&gt;&lt;em&gt;Erret Bishop: Reflections on Him and His Research&lt;/em&gt;&lt;/a&gt; includes a facsimile.)&lt;/p&gt;

&lt;p&gt;Lest anyone else be tempted to conjure the ancient magic of inter-library loan, here is a scan of the pages I borrowed. Thanks to the University of Toledo for supplying the hard copy.&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt; &lt;a href="/img/sicm.pdf"&gt;prl.ccs.neu.edu/img/sicm.pdf&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The document is Bishop&amp;rsquo;s &amp;ldquo;feeling for the philosophical issues involved&amp;rdquo; in constructive mathematics. First, Bishop lists &amp;ldquo;schizophrenic attributes&amp;rdquo; (trouble spots) of contemporary mathematics. Next, he gives basic principles of constructivism and Brouwer&amp;rsquo;s interpretation of the logical quantifiers. Along the way, and as a source of examples, Bishop describes integers, sets, and real numbers. The emphasis is always on common-sense meaning and finite constructions.&lt;/p&gt;

&lt;p&gt;After a brief summary and reflection, the last ten pages list recent advances in constructive mathematics and upcoming tasks. The open tasks are particularly interesting:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;systematically develop (constructive) algebra&lt;/li&gt;
 &lt;li&gt;give a constructive foundation for general topology&lt;/li&gt;
 &lt;li&gt;engage with the deeper &amp;ldquo;meaning of mathematics&amp;rdquo;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;The popular quote on &amp;ldquo;Meaningful Distinctions&amp;rdquo; appears early in the paper, as one of Bishop&amp;rsquo;s four principles that &amp;ldquo;stand out as basic&amp;rdquo; to the philosophy of constructivism:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;A. Mathematics is common sense.&lt;/p&gt;
 &lt;p&gt;B. Do not ask whether a statement is true until you know what it means.&lt;/p&gt;
 &lt;p&gt;C. A proof is any completely convincing argument.&lt;/p&gt;
 &lt;p&gt;D. Meaningful distinctions deserve to be maintained.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I had no idea that D was &amp;ldquo;a principle&amp;rdquo;, or that it had three siblings.&lt;/p&gt;

&lt;p&gt;To further tempt you into reading the whole truth, here are some of my favorite phrases:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;ul&gt;
  &lt;li&gt;One suspects that the majority of pure mathematicians &amp;hellip; ignore as much content as they possibly can.&lt;/li&gt;
  &lt;li&gt;We have geared ourselves to producing research mathematicians who will begin to write papers as soon as possible.  This anti-social and anti-intellectual process defeats even its own narrow ends.&lt;/li&gt;
  &lt;li&gt;&amp;hellip; truth is not a source of trouble to the constructivist, because of his emphasis on meaning.&lt;/li&gt;
  &lt;li&gt;&amp;hellip; guided primarily by considerations of content rather than elegance and formal attractiveness &amp;hellip;&lt;/li&gt;
  &lt;li&gt;Let me tell you what a smart sequence will do.&lt;/li&gt;
  &lt;li&gt;Classical mathematics fails to observe meaningful distinctions having to do with integers.&lt;/li&gt;
  &lt;li&gt;Constructive mathematics does not postulate a pre-existent universe, with objects lying around waiting to be collected and grouped into sets, like shells on a beach.&lt;/li&gt;
  &lt;li&gt;It might be worthwhile to investigate the possibility that constructive mathematics would afford a solid philosophical basis for the theory of computation &amp;hellip;&lt;/li&gt;
  &lt;li&gt;&amp;hellip; if the product of two real numbers is 0, we are not entitled to conclude that one of them is 0.&lt;/li&gt;
  &lt;li&gt;It is fair to say that almost nobody finds his proof intelligible.&lt;/li&gt;
  &lt;li&gt;Mathematics is such a complicated activity that disagreements are bound to arise.&lt;/li&gt;
  &lt;li&gt;Algebraic topology, at least on the elementary level, should not be too difficult to constructivize.&lt;/li&gt;
  &lt;li&gt;I hope all this accords with your common sense, as it does with mine.&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;

&lt;p&gt;Now go find their context!&lt;/p&gt;</description></item>
  <item>
   <title>History of Actors</title>
   <link>http://prl.ccs.neu.edu/blog/2016/10/19/history-of-actors/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2016-10-19-history-of-actors</guid>
   <pubDate>Wed, 19 Oct 2016 17:26:16 UT</pubDate>
   <description>
&lt;p&gt;Christos Dimoulas is currently teaching a &lt;a href="http://www.seas.harvard.edu/courses/cs252/2016fa/"&gt;&amp;ldquo;History of Programming Languages&amp;rdquo; class at Harvard&lt;/a&gt;. The class is, as Christos writes, &amp;ldquo;definitely not about &lt;a href="https://www.levenez.com/lang/lang_letter.pdf"&gt;this&lt;/a&gt;&amp;rdquo;; instead, each meeting is a deep examination of a single, mature research topic, in terms of three to five key papers from the literature.&lt;/p&gt;

&lt;p&gt;On Monday, I presented &amp;ldquo;the History of Actors&amp;rdquo; for the class. I&amp;rsquo;ve made the written-out talk notes and an annotated bibliography available &lt;a href="https://eighty-twenty.org/2016/10/18/actors-hopl"&gt;here&lt;/a&gt;.&lt;/p&gt;</description></item>
  <item>
   <title>Emacs daemon for fast editor startup</title>
   <link>http://prl.ccs.neu.edu/blog/2016/10/17/emacs-daemon-for-fast-editor-startup/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2016-10-17-emacs-daemon-for-fast-editor-startup</guid>
   <pubDate>Mon, 17 Oct 2016 21:48:25 UT</pubDate>
   <description>
&lt;p&gt;In the early days of the famous Emacs/Vim debates, Emacs was often ridiculed for its bulkiness (Eight Megabytes-of-RAM And Constantly Swapping, etc.). The computational power of our computer has grown much faster than Emacs&amp;rsquo; bloat: it takes exactly one second to load on my machine. However, our workflows have also changed, and my workflow implies frequently starting new text editors &amp;mdash; on each git commit for example, or when I use a &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/its-all-text/"&gt;Firefox extension&lt;/a&gt; to edit a textarea content in a proper editor.&lt;/p&gt;

&lt;p&gt;In this blog post, I describe how to use &lt;code&gt;emacsclient&lt;/code&gt; to reuse an existing Emacs process when creating a new editor window, which reduces editor startup times from 1s to 0.150s on my machine.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;Emacs has long supported a client/server mode: a daemon emacs instance is loaded in the background, and whenever you request a new emacs window you can creates a new frame (~ window) using the same instance. This means that the startup time is dramatically reduced after the daemon is launched, as for example the execution of your personal configuration code does not have to be repeated.&lt;/p&gt;

&lt;p&gt;To use it, I have this code as &lt;code&gt;/usr/bin/editor&lt;/code&gt;:&lt;/p&gt;

&lt;div class="brush: sh"&gt;
 &lt;pre&gt;&lt;code&gt;#!/bin/bash
emacsclient -a "" -c "$@"&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The empty &lt;code&gt;-a&lt;/code&gt; parameter means that if no daemon exists, it should start one in the background and retry. The &lt;code&gt;-c&lt;/code&gt; option means that a new frame (window) should be created instead of reusing an existing one. &lt;code&gt;"$@"&lt;/code&gt;means that when the script is invoked with a path as command-line parameter (&lt;code&gt;editor /tmp/foo.txt&lt;/code&gt;), the corresponding file will be opened.&lt;/p&gt;

&lt;p&gt;Finally, my &lt;code&gt;.bash_profile&lt;/code&gt; sets the &lt;code&gt;EDITOR&lt;/code&gt; variable to &lt;code&gt;editor&lt;/code&gt; (&lt;code&gt;export EDITOR=/usr/bin/editor&lt;/code&gt;); this environment variable is what most tools (git included) will use to invoke a text editor.&lt;/p&gt;

&lt;p&gt;On my machine, starting the daemon takes 1.4s. Creating a client windows takes around 0.150s.&lt;/p&gt;

&lt;p&gt;If you want to control the environment in which the daemon process is started, you can launch it explicitly by running &lt;code&gt;emacs --daemon&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Cool kids use &lt;a href="http://spacemacs.org/"&gt;Spacemacs&lt;/a&gt; these days, which comes with all sort of convenient settings built in, and I&amp;rsquo;m told that it does daemonization out of the box. I haven&amp;rsquo;t taken the time to give Spacemacs a try yet.&lt;/p&gt;

&lt;p&gt;Finally, sometimes having all editor windows share the same process is not the right thing, because I do stuff which makes Emacs a bit unstable. (It&amp;rsquo;s not very good at asynchronous communication with the rest of the world, so for example accessing a file through SSH from Emacs can hang the process when network goes bad.). I&amp;rsquo;ve been bitten a few times by a crash of all editor windows at the same time, and since then, when I know I&amp;rsquo;m going to do &amp;ldquo;heavy stuff&amp;rdquo;, I launch a separate process for it (just &lt;code&gt;emacs&lt;/code&gt; instead of &lt;code&gt;editor&lt;/code&gt; or &lt;code&gt;emacsclient&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;P.S.: To precisely measure the startup time, ask Emacs to evaluate a Lisp expression on startup, to kill it immediately.:&lt;/p&gt;

&lt;div class="brush: sh"&gt;
 &lt;pre&gt;&lt;code&gt;$ time emacs --eval "(save-buffers-kill-terminal)"
$ time emacsclient -a '' -c -e "(save-buffers-kill-terminal)"&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description></item>
  <item>
   <title>CompCert Overview</title>
   <link>http://prl.ccs.neu.edu/blog/2016/10/11/compcert-overview/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2016-10-11-compcert-overview</guid>
   <pubDate>Tue, 11 Oct 2016 17:41:16 UT</pubDate>
   <description>
&lt;p&gt;If you are interested in learning about the &lt;em&gt;internals&lt;/em&gt; of the CompCert C compiler but would rather not read its source code, this post is for you.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;(This is a public service announcement.)&lt;/p&gt;

&lt;p&gt;Last fall, I gave a short lecture on the 2006 paper &lt;a href="http://gallium.inria.fr/~xleroy/publi/compiler-certif.pdf"&gt;&amp;ldquo;Formal Certification of a Compiler Back-End&amp;rdquo;&lt;/a&gt; by Xavier Leroy for Amal Ahmed&amp;rsquo;s &lt;a href="http://www.ccs.neu.edu/home/amal/course/7480-f15/"&gt;&amp;ldquo;Special Topics in Programming Languages&amp;rdquo;&lt;/a&gt; class. Rather than present CompCert as it existed in 2006, I read the documentation and source code for &lt;a href="https://github.com/AbsInt/CompCert/releases/tag/v2.5"&gt;CompCert 2.5&lt;/a&gt; (released June 2015). The lecture then focused on three questions:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;What subset of C does CompCert handle, today?&lt;/li&gt;
 &lt;li&gt;What optimizing passes does CompCert perform?&lt;/li&gt;
 &lt;li&gt;What is the &amp;ldquo;correctness theorem&amp;rdquo; for CompCert, and what does this theorem mean?&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;My notes for the lecture give a &amp;ldquo;mid-level&amp;rdquo; summary of the compiler &amp;mdash; there are more details than you&amp;rsquo;ll find in papers, but it&amp;rsquo;s (hopefully!) easier to read than the source code. The document is also hyperlinked to locations in the &lt;a href="https://github.com/AbsInt/CompCert"&gt;CompCert GitHub repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is the document:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt; &lt;a href="http://www.ccs.neu.edu/home/types/resources/notes/compcert/cc.pdf"&gt;http://www.ccs.neu.edu/home/types/resources/notes/compcert/cc.pdf&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;And here is a table-of-contents:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Motivation, details of the source and target languages, high-level guarantees&lt;/li&gt;
 &lt;li&gt;Compiler pipeline, optimizing passes, links intermediate language grammars and Coq theorems&lt;/li&gt;
 &lt;li&gt;Background on compiler correctness&lt;/li&gt;
 &lt;li&gt;CompCert&amp;rsquo;s correctness, properties that CompCert does &lt;strong&gt;not&lt;/strong&gt; guarantee&lt;/li&gt;
 &lt;li&gt;Recent (2006 &amp;ndash; 2015) work in the CompCert ecosystem&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;The document ends with a short description of two other research projects that have grown into &amp;ldquo;industry software&amp;rdquo; and a link to Xaver Leroy&amp;rsquo;s &lt;a href="https://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html"&gt;OPLSS lectures on certified compilers&lt;/a&gt;. Enjoy!&lt;/p&gt;</description></item></channel></rss>